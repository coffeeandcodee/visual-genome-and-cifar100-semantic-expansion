{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d59a44",
   "metadata": {},
   "source": [
    "# CREATING AND ANALYSING EXISTING vg_text NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3c5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File vg_text.txt already exists. Skipping processing.\n",
      "Loaded text: 154198751 characters\n",
      "Tokenized: 33662592 tokens\n",
      "Sample tokens: ['impressive', 'matriss', 'injection', 'siluette', 'multi', 'haltar', 'fisheye', 'trepidation', 'trow', 'maba', 'trysil', 'antacid', 'coloron', 'hesperia', 'winnebego', 'cubical', 'overdoor', 'pickel', 'wiht', 'proffessional']\n",
      "Replaced 62407 rare tokens (threshold=0.00025)\n",
      "Final vocabulary: 458 unique tokens\n",
      "Sample tokens: ['walking', 'tan', 'open', 'pile', 'cheese', 'up', 'fur', 'frisbee', 'dark', 'surfboard', 'oven', 'broccoli', 'remote', 'statue', 'branch', 'four', 'pole', 'sitting', 'and', 'baby']\n",
      "Graph: 458 nodes, 50127 edges\n",
      "Top tokens by frequency:\n",
      "   1. '.' (freq=6085975)\n",
      "   2. '<RARE>' (freq=4416324)\n",
      "   3. 'a' (freq=2220903)\n",
      "   4. 'the' (freq=2155082)\n",
      "   5. 'on' (freq=1396037)\n",
      "   6. 'of' (freq=980462)\n",
      "   7. 'is' (freq=787909)\n",
      "   8. 'in' (freq=714867)\n",
      "   9. 'white' (freq=652421)\n",
      "  10. 'black' (freq=398632)\n",
      "  11. 'and' (freq=341934)\n",
      "  12. 'man' (freq=317885)\n",
      "  13. 'with' (freq=292771)\n",
      "  14. 'blue' (freq=283594)\n",
      "  15. 'red' (freq=237296)\n",
      "\n",
      "✓ Network built: 458 nodes, 50,127 edges\n"
     ]
    }
   ],
   "source": [
    "# NOTE Creating vg network\n",
    "\n",
    "from lab6 import prepare_visual_genome_text, train_embeddings\n",
    "from lab2 import process_text_network, visualize_network\n",
    "\n",
    "zip_url = \"https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset/region_descriptions.json.zip\"\n",
    "text_file = prepare_visual_genome_text(zip_url)\n",
    "\n",
    "\n",
    "vg_network_data = process_text_network(\n",
    "    text_file,\n",
    "    rare_threshold=0.00025,  # Keep only very common tokens\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\n✓ Network built: {vg_network_data['graph'].number_of_nodes():,} nodes, \"\n",
    "      f\"{vg_network_data['graph'].number_of_edges():,} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f8efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WORD FREQUENCIES IN NETWORK\n",
      "================================================================================\n",
      "Total unique words: 458\n",
      "Total token occurrences: 33,662,592\n",
      "\n",
      "Rank   Word                    Frequency % of Total\n",
      "--------------------------------------------------\n",
      "1      .                       6,085,975     18.08%\n",
      "2      <RARE>                  4,416,324     13.12%\n",
      "3      a                       2,220,903      6.60%\n",
      "4      the                     2,155,082      6.40%\n",
      "5      on                      1,396,037      4.15%\n",
      "6      of                        980,462      2.91%\n",
      "7      is                        787,909      2.34%\n",
      "8      in                        714,867      2.12%\n",
      "9      white                     652,421      1.94%\n",
      "10     black                     398,632      1.18%\n",
      "11     and                       341,934      1.02%\n",
      "12     man                       317,885      0.94%\n",
      "13     with                      292,771      0.87%\n",
      "14     blue                      283,594      0.84%\n",
      "15     red                       237,296      0.70%\n",
      "16     green                     232,924      0.69%\n",
      "17     wearing                   210,284      0.62%\n",
      "18     brown                     185,791      0.55%\n",
      "19     building                  161,348      0.48%\n",
      "20     are                       148,331      0.44%\n",
      "21     person                    144,892      0.43%\n",
      "22     woman                     138,752      0.41%\n",
      "23     this                      136,604      0.41%\n",
      "24     wall                      132,768      0.39%\n",
      "25     sky                       129,621      0.39%\n",
      "26     window                    128,777      0.38%\n",
      "27     yellow                    128,186      0.38%\n",
      "28     shirt                     125,364      0.37%\n",
      "29     sign                      122,881      0.37%\n",
      "30     water                     115,216      0.34%\n",
      "31     table                     114,804      0.34%\n",
      "32     to                        113,846      0.34%\n",
      "33     has                       107,963      0.32%\n",
      "34     tree                      104,986      0.31%\n",
      "35     light                     101,409      0.30%\n",
      "36     train                     100,584      0.30%\n",
      "37     two                        99,929      0.30%\n",
      "38     grass                      94,509      0.28%\n",
      "39     an                         94,116      0.28%\n",
      "40     side                       91,302      0.27%\n",
      "41     large                      90,915      0.27%\n",
      "42     small                      90,656      0.27%\n",
      "43     street                     82,796      0.25%\n",
      "44     front                      82,557      0.25%\n",
      "45     ground                     82,356      0.24%\n",
      "46     top                        81,962      0.24%\n",
      "47     plate                      79,913      0.24%\n",
      "48     car                        74,000      0.22%\n",
      "49     part                       68,927      0.20%\n",
      "50     orange                     67,542      0.20%\n",
      "51     head                       67,285      0.20%\n",
      "52     clouds                     64,941      0.19%\n",
      "53     wooden                     64,547      0.19%\n",
      "54     standing                   64,532      0.19%\n",
      "55     bus                        64,424      0.19%\n",
      "56     pole                       63,624      0.19%\n",
      "57     sitting                    62,746      0.19%\n",
      "58     metal                      62,422      0.19%\n",
      "59     behind                     62,322      0.19%\n",
      "60     holding                    62,133      0.18%\n",
      "61     color                      61,149      0.18%\n",
      "62     trees                      61,061      0.18%\n",
      "63     silver                     60,315      0.18%\n",
      "64     snow                       59,851      0.18%\n",
      "65     gray                       59,768      0.18%\n",
      "66     people                     59,339      0.18%\n",
      "67     dog                        58,131      0.17%\n",
      "68     hand                       57,631      0.17%\n",
      "69     road                       56,377      0.17%\n",
      "70     tennis                     56,183      0.17%\n",
      "71     hair                       55,250      0.16%\n",
      "72     grey                       54,231      0.16%\n",
      "73     dark                       53,772      0.16%\n",
      "74     glass                      53,077      0.16%\n",
      "75     at                         50,412      0.15%\n",
      "76     plane                      50,195      0.15%\n",
      "77     back                       49,939      0.15%\n",
      "78     floor                      49,592      0.15%\n",
      "79     cat                        48,978      0.15%\n",
      "80     background                 48,850      0.15%\n",
      "81     fence                      48,722      0.14%\n",
      "82     clock                      48,573      0.14%\n",
      "83     door                       47,960      0.14%\n",
      "84     giraffe                    47,524      0.14%\n",
      "85     leaves                     45,936      0.14%\n",
      "86     boy                        45,489      0.14%\n",
      "87     left                       45,460      0.14%\n",
      "88     field                      45,217      0.13%\n",
      "89     right                      44,825      0.13%\n",
      "90     next                       44,805      0.13%\n",
      "91     long                       44,176      0.13%\n",
      "92     by                         43,611      0.13%\n",
      "93     bear                       43,395      0.13%\n",
      "94     ,                          43,354      0.13%\n",
      "95     chair                      42,202      0.13%\n",
      "96     elephant                   41,969      0.12%\n",
      "97     tall                       41,703      0.12%\n",
      "98     pink                       41,374      0.12%\n",
      "99     man's                      41,303      0.12%\n",
      "100    girl                       40,839      0.12%\n",
      "101    horse                      40,598      0.12%\n",
      "102    pizza                      40,518      0.12%\n",
      "103    for                        39,936      0.12%\n",
      "104    baseball                   38,871      0.12%\n",
      "105    zebra                      37,684      0.11%\n",
      "106    pants                      37,335      0.11%\n",
      "107    hat                        37,304      0.11%\n",
      "108    boat                       37,051      0.11%\n",
      "109    truck                      36,587      0.11%\n",
      "110    walking                    36,006      0.11%\n",
      "111    jacket                     35,584      0.11%\n",
      "112    bench                      35,541      0.11%\n",
      "113    sidewalk                   35,170      0.10%\n",
      "114    it                         34,445      0.10%\n",
      "115    hanging                    34,310      0.10%\n",
      "116    toilet                     34,210      0.10%\n",
      "117    up                         33,627      0.10%\n",
      "118    bird                       33,307      0.10%\n",
      "119    tail                       32,779      0.10%\n",
      "120    bed                        32,516      0.10%\n",
      "121    parked                     32,463      0.10%\n",
      "122    down                       32,307      0.10%\n",
      "123    near                       32,142      0.10%\n",
      "124    wood                       31,304      0.09%\n",
      "125    board                      31,234      0.09%\n",
      "126    from                       30,956      0.09%\n",
      "127    colored                    30,904      0.09%\n",
      "128    ear                        30,667      0.09%\n",
      "129    windows                    30,183      0.09%\n",
      "130    clear                      29,817      0.09%\n",
      "131    face                       29,692      0.09%\n",
      "132    motorcycle                 29,633      0.09%\n",
      "133    under                      29,597      0.09%\n",
      "134    umbrella                   29,587      0.09%\n",
      "135    leg                        29,111      0.09%\n",
      "136    player                     28,951      0.09%\n",
      "137    line                       28,620      0.09%\n",
      "138    over                       27,938      0.08%\n",
      "139    helmet                     27,627      0.08%\n",
      "140    picture                    27,490      0.08%\n",
      "141    food                       27,386      0.08%\n",
      "142    eye                        27,364      0.08%\n",
      "143    bag                        26,878      0.08%\n",
      "144    beach                      26,674      0.08%\n",
      "145    nose                       26,655      0.08%\n",
      "146    bowl                       26,525      0.08%\n",
      "147    handle                     26,516      0.08%\n",
      "148    brick                      26,512      0.08%\n",
      "149    air                        26,307      0.08%\n",
      "150    paper                      26,138      0.08%\n",
      "151    shadow                     26,009      0.08%\n",
      "152    cow                        25,779      0.08%\n",
      "153    skateboard                 25,755      0.08%\n",
      "154    three                      25,681      0.08%\n",
      "155    shorts                     25,587      0.08%\n",
      "156    tracks                     25,262      0.08%\n",
      "157    bottle                     25,051      0.07%\n",
      "158    bike                       25,042      0.07%\n",
      "159    that                       24,679      0.07%\n",
      "160    sheep                      24,665      0.07%\n",
      "161    ball                       24,495      0.07%\n",
      "162    lights                     23,675      0.07%\n",
      "163    wheel                      23,472      0.07%\n",
      "164    short                      23,274      0.07%\n",
      "165    letter                     23,105      0.07%\n",
      "166    round                      23,094      0.07%\n",
      "167    above                      23,089      0.07%\n",
      "168    number                     23,056      0.07%\n",
      "169    tan                        23,006      0.07%\n",
      "170    plastic                    22,795      0.07%\n",
      "171    edge                       22,779      0.07%\n",
      "172    open                       22,768      0.07%\n",
      "173    his                        22,729      0.07%\n",
      "174    kite                       22,632      0.07%\n",
      "175    box                        22,479      0.07%\n",
      "176    phone                      22,459      0.07%\n",
      "177    dirt                       22,452      0.07%\n",
      "178    distance                   22,449      0.07%\n",
      "179    ski                        22,362      0.07%\n",
      "180    shoes                      22,312      0.07%\n",
      "181    flowers                    22,200      0.07%\n",
      "182    surfboard                  22,177      0.07%\n",
      "183    cake                       22,087      0.07%\n",
      "184    piece                      21,557      0.06%\n",
      "185    around                     21,551      0.06%\n",
      "186    desk                       21,540      0.06%\n",
      "187    airplane                   21,491      0.06%\n",
      "188    sink                       21,452      0.06%\n",
      "189    mirror                     21,412      0.06%\n",
      "190    counter                    21,346      0.06%\n",
      "191    house                      21,186      0.06%\n",
      "192    post                       21,135      0.06%\n",
      "193    traffic                    21,116      0.06%\n",
      "194    made                       20,998      0.06%\n",
      "195    arm                        20,851      0.06%\n",
      "196    woman's                    20,763      0.06%\n",
      "197    laptop                     20,702      0.06%\n",
      "198    pair                       20,529      0.06%\n",
      "199    computer                   20,505      0.06%\n",
      "200    ocean                      20,467      0.06%\n",
      "201    purple                     20,377      0.06%\n",
      "202    sand                       20,292      0.06%\n",
      "203    lamp                       20,193      0.06%\n",
      "204    young                      20,153      0.06%\n",
      "205    glasses                    20,135      0.06%\n",
      "206    big                        19,820      0.06%\n",
      "207    legs                       19,804      0.06%\n",
      "208    bathroom                   19,754      0.06%\n",
      "209    covered                    19,532      0.06%\n",
      "210    looking                    19,414      0.06%\n",
      "211    cup                        19,129      0.06%\n",
      "212    photo                      19,082      0.06%\n",
      "213    flower                     18,958      0.06%\n",
      "214    one                        18,851      0.06%\n",
      "215    roof                       18,690      0.06%\n",
      "216    vase                       18,615      0.06%\n",
      "217    cap                        18,382      0.05%\n",
      "218    riding                     18,188      0.05%\n",
      "219    tower                      18,164      0.05%\n",
      "220    out                        18,122      0.05%\n",
      "221    lines                      17,875      0.05%\n",
      "222    shelf                      17,810      0.05%\n",
      "223    trunk                      17,763      0.05%\n",
      "224    some                       17,484      0.05%\n",
      "225    tire                       17,444      0.05%\n",
      "226    shoe                       17,063      0.05%\n",
      "227    area                       17,055      0.05%\n",
      "228    jeans                      16,865      0.05%\n",
      "229    playing                    16,854      0.05%\n",
      "230    there                      16,763      0.05%\n",
      "231    racket                     16,691      0.05%\n",
      "232    logo                       16,111      0.05%\n",
      "233    court                      16,102      0.05%\n",
      "234    tile                       16,098      0.05%\n",
      "235    painted                    15,634      0.05%\n",
      "236    rock                       15,503      0.05%\n",
      "237    flying                     15,417      0.05%\n",
      "238    room                       15,407      0.05%\n",
      "239    bottom                     15,284      0.05%\n",
      "240    neck                       15,281      0.05%\n",
      "241    seat                       15,253      0.05%\n",
      "242    stone                      15,227      0.05%\n",
      "243    frisbee                    15,180      0.05%\n",
      "244    group                      15,115      0.04%\n",
      "245    bright                     14,961      0.04%\n",
      "246    hydrant                    14,855      0.04%\n",
      "247    child                      14,731      0.04%\n",
      "248    mouth                      14,630      0.04%\n",
      "249    stop                       14,627      0.04%\n",
      "250    fire                       14,619      0.04%\n",
      "251    outside                    14,607      0.04%\n",
      "252    baby                       14,523      0.04%\n",
      "253    kitchen                    14,476      0.04%\n",
      "254    reflection                 14,466      0.04%\n",
      "255    tie                        14,458      0.04%\n",
      "256    plant                      14,340      0.04%\n",
      "257    patch                      14,333      0.04%\n",
      "258    wave                       14,245      0.04%\n",
      "259    foot                       14,161      0.04%\n",
      "260    little                     14,154      0.04%\n",
      "261    ceiling                    14,055      0.04%\n",
      "262    body                       13,984      0.04%\n",
      "263    eyes                       13,978      0.04%\n",
      "264    striped                    13,972      0.04%\n",
      "265    keyboard                   13,922      0.04%\n",
      "266    hill                       13,922      0.04%\n",
      "267    laying                     13,902      0.04%\n",
      "268    mountain                   13,745      0.04%\n",
      "269    lady                       13,672      0.04%\n",
      "270    concrete                   13,651      0.04%\n",
      "271    pillow                     13,566      0.04%\n",
      "272    these                      13,544      0.04%\n",
      "273    old                        13,513      0.04%\n",
      "274    flag                       13,356      0.04%\n",
      "275    inside                     13,318      0.04%\n",
      "276    gold                       13,200      0.04%\n",
      "277    vehicle                    13,084      0.04%\n",
      "278    four                       13,042      0.04%\n",
      "279    hands                      13,034      0.04%\n",
      "280    teddy                      12,956      0.04%\n",
      "281    coat                       12,881      0.04%\n",
      "282    can                        12,865      0.04%\n",
      "283    wheels                     12,827      0.04%\n",
      "284    letters                    12,797      0.04%\n",
      "285    parking                    12,682      0.04%\n",
      "286    leaf                       12,620      0.04%\n",
      "287    rocks                      12,601      0.04%\n",
      "288    banana                     12,588      0.04%\n",
      "289    suit                       12,581      0.04%\n",
      "290    stripes                    12,563      0.04%\n",
      "291    couch                      12,503      0.04%\n",
      "292    no                         12,374      0.04%\n",
      "293    stripe                     12,353      0.04%\n",
      "294    writing                    12,340      0.04%\n",
      "295    frame                      12,214      0.04%\n",
      "296    eating                     12,192      0.04%\n",
      "297    bat                        12,096      0.04%\n",
      "298    row                        12,080      0.04%\n",
      "299    waves                      12,078      0.04%\n",
      "300    wine                       12,037      0.04%\n",
      "301    her                        12,005      0.04%\n",
      "302    corner                     11,990      0.04%\n",
      "303    off                        11,968      0.04%\n",
      "304    wing                       11,929      0.04%\n",
      "305    camera                     11,921      0.04%\n",
      "306    stand                      11,771      0.03%\n",
      "307    screen                     11,744      0.03%\n",
      "308    cars                       11,704      0.03%\n",
      "309    along                      11,599      0.03%\n",
      "310    book                       11,555      0.03%\n",
      "311    sandwich                   11,525      0.03%\n",
      "312    animal                     11,459      0.03%\n",
      "313    towel                      11,434      0.03%\n",
      "314    coffee                     11,419      0.03%\n",
      "315    wet                        11,410      0.03%\n",
      "316    beside                     11,279      0.03%\n",
      "317    bicycle                    11,238      0.03%\n",
      "318    men                        11,135      0.03%\n",
      "319    fork                       11,010      0.03%\n",
      "320    cabinet                    10,954      0.03%\n",
      "321    empty                      10,882      0.03%\n",
      "322    knife                      10,850      0.03%\n",
      "323    square                     10,840      0.03%\n",
      "324    container                  10,810      0.03%\n",
      "325    skier                      10,764      0.03%\n",
      "326    sunglasses                 10,756      0.03%\n",
      "327    park                       10,742      0.03%\n",
      "328    feet                       10,739      0.03%\n",
      "329    spot                       10,723      0.03%\n",
      "330    t                          10,561      0.03%\n",
      "331    city                       10,556      0.03%\n",
      "332    pot                        10,548      0.03%\n",
      "333    attached                   10,492      0.03%\n",
      "334    design                     10,443      0.03%\n",
      "335    basket                     10,386      0.03%\n",
      "336    very                       10,268      0.03%\n",
      "337    cheese                     10,250      0.03%\n",
      "338    buildings                  10,214      0.03%\n",
      "339    button                     10,195      0.03%\n",
      "340    against                    10,078      0.03%\n",
      "341    dress                      10,074      0.03%\n",
      "342    surfer                     10,073      0.03%\n",
      "343    guy                        10,066      0.03%\n",
      "344    stove                      10,011      0.03%\n",
      "345    slice                      10,007      0.03%\n",
      "346    suitcase                    9,999      0.03%\n",
      "347    engine                      9,923      0.03%\n",
      "348    bananas                     9,913      0.03%\n",
      "349    growing                     9,886      0.03%\n",
      "350    game                        9,809      0.03%\n",
      "351    poles                       9,775      0.03%\n",
      "352    bush                        9,726      0.03%\n",
      "353    wrist                       9,696      0.03%\n",
      "354    bridge                      9,680      0.03%\n",
      "355    tray                        9,679      0.03%\n",
      "356    many                        9,670      0.03%\n",
      "357    cloud                       9,561      0.03%\n",
      "358    cell                        9,555      0.03%\n",
      "359    middle                      9,483      0.03%\n",
      "360    ears                        9,474      0.03%\n",
      "361    signs                       9,453      0.03%\n",
      "362    through                     9,443      0.03%\n",
      "363    oven                        9,418      0.03%\n",
      "364    windshield                  9,399      0.03%\n",
      "365    track                       9,323      0.03%\n",
      "366    hot                         9,243      0.03%\n",
      "367    van                         9,193      0.03%\n",
      "368    carrying                    9,119      0.03%\n",
      "369    glove                       9,074      0.03%\n",
      "370    lot                         9,042      0.03%\n",
      "371    scene                       9,035      0.03%\n",
      "372    watching                    8,954      0.03%\n",
      "373    other                       8,918      0.03%\n",
      "374    elephants                   8,908      0.03%\n",
      "375    bread                       8,890      0.03%\n",
      "376    surface                     8,864      0.03%\n",
      "377    apple                       8,852      0.03%\n",
      "378    mouse                       8,793      0.03%\n",
      "379    section                     8,774      0.03%\n",
      "380    view                        8,754      0.03%\n",
      "381    zebras                      8,731      0.03%\n",
      "382    full                        8,719      0.03%\n",
      "383    cloudy                      8,713      0.03%\n",
      "384    chain                       8,713      0.03%\n",
      "385    end                         8,694      0.03%\n",
      "386    broccoli                    8,694      0.03%\n",
      "387    base                        8,679      0.03%\n",
      "388    bunch                       8,675      0.03%\n",
      "389    giraffes                    8,667      0.03%\n",
      "390    sun                         8,665      0.03%\n",
      "391    several                     8,639      0.03%\n",
      "392    fruit                       8,626      0.03%\n",
      "393    tv                          8,587      0.03%\n",
      "394    between                     8,491      0.03%\n",
      "395    being                       8,477      0.03%\n",
      "396    trash                       8,473      0.03%\n",
      "397    wears                       8,467      0.03%\n",
      "398    lid                         8,436      0.03%\n",
      "399    worn                        8,313      0.02%\n",
      "400    skis                        8,284      0.02%\n",
      "401    railing                     8,218      0.02%\n",
      "402    colorful                    8,178      0.02%\n",
      "403    jet                         8,176      0.02%\n",
      "404    blanket                     8,175      0.02%\n",
      "405    dish                        8,113      0.02%\n",
      "406    cloth                       8,097      0.02%\n",
      "407    branches                    8,079      0.02%\n",
      "408    branch                      8,022      0.02%\n",
      "409    rear                        8,001      0.02%\n",
      "410    visible                     7,987      0.02%\n",
      "411    collar                      7,976      0.02%\n",
      "412    watch                       7,961      0.02%\n",
      "413    bushes                      7,937      0.02%\n",
      "414    remote                      7,892      0.02%\n",
      "415    pavement                    7,812      0.02%\n",
      "416    wire                        7,795      0.02%\n",
      "417    cover                       7,723      0.02%\n",
      "418    set                         7,711      0.02%\n",
      "419    beige                       7,698      0.02%\n",
      "420    horses                      7,670      0.02%\n",
      "421    passenger                   7,655      0.02%\n",
      "422    fur                         7,599      0.02%\n",
      "423    blonde                      7,575      0.02%\n",
      "424    into                        7,573      0.02%\n",
      "425    donut                       7,538      0.02%\n",
      "426    platform                    7,525      0.02%\n",
      "427    display                     7,494      0.02%\n",
      "428    backpack                    7,469      0.02%\n",
      "429    tag                         7,432      0.02%\n",
      "430    napkin                      7,361      0.02%\n",
      "431    cement                      7,353      0.02%\n",
      "432    sweater                     7,321      0.02%\n",
      "433    monitor                     7,316      0.02%\n",
      "434    curtain                     7,313      0.02%\n",
      "435    stuffed                     7,309      0.02%\n",
      "436    tank                        7,281      0.02%\n",
      "437    closed                      7,251      0.02%\n",
      "438    pile                        7,235      0.02%\n",
      "439    word                        7,189      0.02%\n",
      "440    have                        7,179      0.02%\n",
      "441    rack                        7,179      0.02%\n",
      "442    taken                       7,165      0.02%\n",
      "443    mountains                   7,120      0.02%\n",
      "444    sauce                       7,104      0.02%\n",
      "445    luggage                     7,092      0.02%\n",
      "446    cows                        7,040      0.02%\n",
      "447    smiling                     7,037      0.02%\n",
      "448    double                      7,026      0.02%\n",
      "449    skiing                      6,971      0.02%\n",
      "450    he                          6,960      0.02%\n",
      "451    numbers                     6,956      0.02%\n",
      "452    sleeve                      6,949      0.02%\n",
      "453    spoon                       6,946      0.02%\n",
      "454    person's                    6,936      0.02%\n",
      "455    rail                        6,930      0.02%\n",
      "456    lettering                   6,922      0.02%\n",
      "457    statue                      6,917      0.02%\n",
      "458    license                     6,916      0.02%\n",
      "\n",
      "================================================================================\n",
      "Summary: 458 words, 33,662,592 total occurrences\n"
     ]
    }
   ],
   "source": [
    "# NOTE Occurrences per word in network\n",
    "# Display frequency of all words in the network, sorted by frequency\n",
    "\n",
    "token_counts = vg_network_data['token_counts']\n",
    "nodes = vg_network_data['nodes']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WORD FREQUENCIES IN NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total unique words: {len(nodes)}\")\n",
    "print(f\"Total token occurrences: {sum(token_counts.values()):,}\")\n",
    "print()\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "sorted_words = sorted(token_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "# Print all words with their frequencies\n",
    "print(f\"{'Rank':<6} {'Word':<20} {'Frequency':>12} {'% of Total':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total = sum(token_counts.values())\n",
    "for rank, (word, freq) in enumerate(sorted_words, 1):\n",
    "    pct = (freq / total) * 100\n",
    "    print(f\"{rank:<6} {word:<20} {freq:>12,} {pct:>9.2f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"Summary: {len(nodes)} words, {sum(token_counts.values()):,} total occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cac299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EDGES PER NODE (DEGREE)\n",
      "================================================================================\n",
      "Total nodes: 458\n",
      "Total edges: 50127\n",
      "Average degree: 218.90\n",
      "\n",
      "Rank   Word                     Degree   % of Max\n",
      "--------------------------------------------------\n",
      "1      .                           457     100.0%\n",
      "2      <RARE>                      457     100.0%\n",
      "3      is                          457     100.0%\n",
      "4      a                           456      99.8%\n",
      "5      the                         456      99.8%\n",
      "6      on                          455      99.6%\n",
      "7      of                          455      99.6%\n",
      "8      in                          455      99.6%\n",
      "9      and                         455      99.6%\n",
      "10     white                       454      99.3%\n",
      "11     with                        454      99.3%\n",
      "12     ,                           453      99.1%\n",
      "13     to                          452      98.9%\n",
      "14     black                       447      97.8%\n",
      "15     are                         447      97.8%\n",
      "16     behind                      444      97.2%\n",
      "17     over                        444      97.2%\n",
      "18     by                          443      96.9%\n",
      "19     at                          441      96.5%\n",
      "20     red                         440      96.3%\n",
      "21     yellow                      439      96.1%\n",
      "22     for                         439      96.1%\n",
      "23     blue                        438      95.8%\n",
      "24     green                       438      95.8%\n",
      "25     has                         437      95.6%\n",
      "26     small                       437      95.6%\n",
      "27     brown                       434      95.0%\n",
      "28     from                        434      95.0%\n",
      "29     under                       432      94.5%\n",
      "30     large                       430      94.1%\n",
      "31     dark                        425      93.0%\n",
      "32     orange                      424      92.8%\n",
      "33     up                          424      92.8%\n",
      "34     near                        422      92.3%\n",
      "35     colored                     418      91.5%\n",
      "36     grey                        417      91.2%\n",
      "37     above                       417      91.2%\n",
      "38     around                      417      91.2%\n",
      "39     gray                        413      90.4%\n",
      "40     that                        407      89.1%\n",
      "41     two                         406      88.8%\n",
      "42     holding                     404      88.4%\n",
      "43     pink                        398      87.1%\n",
      "44     inside                      398      87.1%\n",
      "45     between                     390      85.3%\n",
      "46     next                        388      84.9%\n",
      "47     one                         388      84.9%\n",
      "48     this                        386      84.5%\n",
      "49     sitting                     383      83.8%\n",
      "50     left                        383      83.8%\n",
      "51     silver                      382      83.6%\n",
      "52     top                         381      83.4%\n",
      "53     right                       380      83.2%\n",
      "54     long                        380      83.2%\n",
      "55     off                         380      83.2%\n",
      "56     side                        378      82.7%\n",
      "57     tan                         378      82.7%\n",
      "58     beside                      378      82.7%\n",
      "59     color                       375      82.1%\n",
      "60     covered                     372      81.4%\n",
      "61     metal                       369      80.7%\n",
      "62     out                         369      80.7%\n",
      "63     looking                     367      80.3%\n",
      "64     light                       366      80.1%\n",
      "65     hanging                     366      80.1%\n",
      "66     big                         366      80.1%\n",
      "67     back                        362      79.2%\n",
      "68     purple                      361      79.0%\n",
      "69     outside                     357      78.1%\n",
      "70     down                        356      77.9%\n",
      "71     standing                    355      77.7%\n",
      "72     against                     355      77.7%\n",
      "73     painted                     346      75.7%\n",
      "74     three                       344      75.3%\n",
      "75     tall                        342      74.8%\n",
      "76     round                       342      74.8%\n",
      "77     front                       339      74.2%\n",
      "78     wooden                      339      74.2%\n",
      "79     wearing                     338      74.0%\n",
      "80     visible                     337      73.7%\n",
      "81     plastic                     335      73.3%\n",
      "82     gold                        333      72.9%\n",
      "83     open                        329      72.0%\n",
      "84     little                      329      72.0%\n",
      "85     colorful                    326      71.3%\n",
      "86     made                        321      70.2%\n",
      "87     through                     321      70.2%\n",
      "88     man                         320      70.0%\n",
      "89     old                         318      69.6%\n",
      "90     sign                        310      67.8%\n",
      "91     have                        307      67.2%\n",
      "92     into                        306      67.0%\n",
      "93     being                       304      66.5%\n",
      "94     beige                       303      66.3%\n",
      "95     some                        302      66.1%\n",
      "96     area                        298      65.2%\n",
      "97     short                       295      64.6%\n",
      "98     can                         293      64.1%\n",
      "99     wood                        292      63.9%\n",
      "100    four                        287      62.8%\n",
      "101    along                       287      62.8%\n",
      "102    his                         286      62.6%\n",
      "103    square                      286      62.6%\n",
      "104    clear                       284      62.1%\n",
      "105    wall                        283      61.9%\n",
      "106    striped                     282      61.7%\n",
      "107    glass                       281      61.5%\n",
      "108    cover                       281      61.5%\n",
      "109    other                       280      61.3%\n",
      "110    bottom                      279      61.1%\n",
      "111    no                          278      60.8%\n",
      "112    full                        277      60.6%\n",
      "113    water                       274      60.0%\n",
      "114    stand                       270      59.1%\n",
      "115    an                          269      58.9%\n",
      "116    laying                      269      58.9%\n",
      "117    head                        266      58.2%\n",
      "118    empty                       266      58.2%\n",
      "119    table                       261      57.1%\n",
      "120    middle                      260      56.9%\n",
      "121    set                         260      56.9%\n",
      "122    train                       257      56.2%\n",
      "123    face                        257      56.2%\n",
      "124    attached                    257      56.2%\n",
      "125    woman                       254      55.6%\n",
      "126    dog                         254      55.6%\n",
      "127    line                        250      54.7%\n",
      "128    hand                        249      54.5%\n",
      "129    board                       249      54.5%\n",
      "130    car                         248      54.3%\n",
      "131    box                         248      54.3%\n",
      "132    design                      248      54.3%\n",
      "133    carrying                    248      54.3%\n",
      "134    window                      244      53.4%\n",
      "135    person                      243      53.2%\n",
      "136    walking                     243      53.2%\n",
      "137    bear                        239      52.3%\n",
      "138    her                         237      51.9%\n",
      "139    building                    236      51.6%\n",
      "140    bus                         234      51.2%\n",
      "141    tree                        233      51.0%\n",
      "142    stone                       233      51.0%\n",
      "143    display                     232      50.8%\n",
      "144    several                     230      50.3%\n",
      "145    street                      229      50.1%\n",
      "146    cat                         229      50.1%\n",
      "147    paper                       229      50.1%\n",
      "148    wet                         229      50.1%\n",
      "149    many                        228      49.9%\n",
      "150    he                          228      49.9%\n",
      "151    word                        226      49.5%\n",
      "152    man's                       225      49.2%\n",
      "153    eating                      225      49.2%\n",
      "154    bike                        224      49.0%\n",
      "155    plate                       223      48.8%\n",
      "156    chair                       223      48.8%\n",
      "157    concrete                    223      48.8%\n",
      "158    pole                        222      48.6%\n",
      "159    riding                      222      48.6%\n",
      "160    worn                        222      48.6%\n",
      "161    door                        221      48.4%\n",
      "162    shadow                      221      48.4%\n",
      "163    watching                    221      48.4%\n",
      "164    food                        220      48.1%\n",
      "165    picture                     219      47.9%\n",
      "166    background                  216      47.3%\n",
      "167    shirt                       215      47.0%\n",
      "168    grass                       215      47.0%\n",
      "169    bench                       215      47.0%\n",
      "170    baby                        215      47.0%\n",
      "171    part                        213      46.6%\n",
      "172    logo                        213      46.6%\n",
      "173    clock                       212      46.4%\n",
      "174    boat                        212      46.4%\n",
      "175    playing                     212      46.4%\n",
      "176    bright                      212      46.4%\n",
      "177    tennis                      210      46.0%\n",
      "178    elephant                    210      46.0%\n",
      "179    bed                         210      46.0%\n",
      "180    ski                         210      46.0%\n",
      "181    cake                        210      46.0%\n",
      "182    closed                      210      46.0%\n",
      "183    snow                        209      45.7%\n",
      "184    horse                       209      45.7%\n",
      "185    truck                       209      45.7%\n",
      "186    ball                        208      45.5%\n",
      "187    flower                      208      45.5%\n",
      "188    handle                      207      45.3%\n",
      "189    flying                      207      45.3%\n",
      "190    motorcycle                  205      44.9%\n",
      "191    woman's                     205      44.9%\n",
      "192    floor                       203      44.4%\n",
      "193    bag                         203      44.4%\n",
      "194    road                        202      44.2%\n",
      "195    seat                        202      44.2%\n",
      "196    base                        202      44.2%\n",
      "197    rear                        202      44.2%\n",
      "198    wire                        202      44.2%\n",
      "199    brick                       201      44.0%\n",
      "200    piece                       200      43.8%\n",
      "201    corner                      200      43.8%\n",
      "202    surface                     199      43.5%\n",
      "203    frame                       198      43.3%\n",
      "204    cement                      198      43.3%\n",
      "205    house                       197      43.1%\n",
      "206    arm                         197      43.1%\n",
      "207    cloth                       197      43.1%\n",
      "208    boy                         196      42.9%\n",
      "209    fence                       195      42.7%\n",
      "210    bird                        194      42.5%\n",
      "211    lights                      193      42.2%\n",
      "212    park                        193      42.2%\n",
      "213    girl                        192      42.0%\n",
      "214    tail                        192      42.0%\n",
      "215    number                      192      42.0%\n",
      "216    animal                      191      41.8%\n",
      "217    hair                        190      41.6%\n",
      "218    baseball                    190      41.6%\n",
      "219    lines                       190      41.6%\n",
      "220    edge                        189      41.4%\n",
      "221    post                        189      41.4%\n",
      "222    kitchen                     189      41.4%\n",
      "223    section                     189      41.4%\n",
      "224    ground                      188      41.1%\n",
      "225    plane                       188      41.1%\n",
      "226    beach                       188      41.1%\n",
      "227    double                      188      41.1%\n",
      "228    foot                        187      40.9%\n",
      "229    body                        187      40.9%\n",
      "230    end                         187      40.9%\n",
      "231    dirt                        186      40.7%\n",
      "232    reflection                  186      40.7%\n",
      "233    giraffe                     185      40.5%\n",
      "234    hat                         185      40.5%\n",
      "235    umbrella                    184      40.3%\n",
      "236    toilet                      182      39.8%\n",
      "237    leg                         182      39.8%\n",
      "238    airplane                    182      39.8%\n",
      "239    bathroom                    182      39.8%\n",
      "240    pizza                       181      39.6%\n",
      "241    windows                     181      39.6%\n",
      "242    tower                       181      39.6%\n",
      "243    wears                       181      39.6%\n",
      "244    field                       180      39.4%\n",
      "245    kite                        180      39.4%\n",
      "246    lamp                        179      39.2%\n",
      "247    bicycle                     179      39.2%\n",
      "248    bowl                        178      38.9%\n",
      "249    skateboard                  178      38.9%\n",
      "250    cap                         177      38.7%\n",
      "251    rock                        177      38.7%\n",
      "252    smiling                     177      38.7%\n",
      "253    cow                         176      38.5%\n",
      "254    traffic                     176      38.5%\n",
      "255    sidewalk                    175      38.3%\n",
      "256    roof                        175      38.3%\n",
      "257    people                      174      38.1%\n",
      "258    zebra                       173      37.9%\n",
      "259    parked                      173      37.9%\n",
      "260    fire                        173      37.9%\n",
      "261    plant                       173      37.9%\n",
      "262    statue                      173      37.9%\n",
      "263    wheel                       172      37.6%\n",
      "264    phone                       172      37.6%\n",
      "265    cup                         172      37.6%\n",
      "266    room                        172      37.6%\n",
      "267    jacket                      171      37.4%\n",
      "268    mirror                      171      37.4%\n",
      "269    stop                        171      37.4%\n",
      "270    banana                      171      37.4%\n",
      "271    container                   171      37.4%\n",
      "272    track                       171      37.4%\n",
      "273    sky                         170      37.2%\n",
      "274    air                         169      37.0%\n",
      "275    legs                        169      37.0%\n",
      "276    flag                        169      37.0%\n",
      "277    parking                     169      37.0%\n",
      "278    platform                    169      37.0%\n",
      "279    trees                       168      36.8%\n",
      "280    computer                    168      36.8%\n",
      "281    leaves                      167      36.5%\n",
      "282    tray                        167      36.5%\n",
      "283    rail                        167      36.5%\n",
      "284    shoes                       166      36.3%\n",
      "285    desk                        165      36.1%\n",
      "286    button                      165      36.1%\n",
      "287    apple                       165      36.1%\n",
      "288    child                       164      35.9%\n",
      "289    shelf                       163      35.7%\n",
      "290    very                        163      35.7%\n",
      "291    it                          162      35.4%\n",
      "292    nose                        162      35.4%\n",
      "293    stripe                      162      35.4%\n",
      "294    city                        162      35.4%\n",
      "295    ear                         161      35.2%\n",
      "296    sand                        161      35.2%\n",
      "297    neck                        161      35.2%\n",
      "298    tie                         161      35.2%\n",
      "299    vehicle                     161      35.2%\n",
      "300    stuffed                     161      35.2%\n",
      "301    rack                        161      35.2%\n",
      "302    pants                       160      35.0%\n",
      "303    sheep                       160      35.0%\n",
      "304    young                       160      35.0%\n",
      "305    coat                        160      35.0%\n",
      "306    screen                      160      35.0%\n",
      "307    bridge                      160      35.0%\n",
      "308    hot                         160      35.0%\n",
      "309    sun                         159      34.8%\n",
      "310    book                        158      34.6%\n",
      "311    feet                        158      34.6%\n",
      "312    luggage                     158      34.6%\n",
      "313    tile                        157      34.4%\n",
      "314    signs                       157      34.4%\n",
      "315    fruit                       157      34.4%\n",
      "316    passenger                   157      34.4%\n",
      "317    eye                         155      33.9%\n",
      "318    laptop                      155      33.9%\n",
      "319    photo                       155      33.9%\n",
      "320    shoe                        155      33.9%\n",
      "321    poles                       155      33.9%\n",
      "322    sink                        154      33.7%\n",
      "323    stripes                     154      33.7%\n",
      "324    player                      153      33.5%\n",
      "325    helmet                      153      33.5%\n",
      "326    trunk                       152      33.3%\n",
      "327    dish                        152      33.3%\n",
      "328    bottle                      150      32.8%\n",
      "329    vase                        150      32.8%\n",
      "330    basket                      150      32.8%\n",
      "331    oven                        150      32.8%\n",
      "332    wine                        148      32.4%\n",
      "333    dress                       148      32.4%\n",
      "334    counter                     147      32.2%\n",
      "335    glasses                     147      32.2%\n",
      "336    surfboard                   146      31.9%\n",
      "337    horses                      146      31.9%\n",
      "338    coffee                      145      31.7%\n",
      "339    cabinet                     145      31.7%\n",
      "340    person's                    145      31.7%\n",
      "341    camera                      143      31.3%\n",
      "342    skiing                      143      31.3%\n",
      "343    flowers                     142      31.1%\n",
      "344    mountain                    142      31.1%\n",
      "345    cars                        142      31.1%\n",
      "346    pot                         142      31.1%\n",
      "347    suitcase                    142      31.1%\n",
      "348    shorts                      141      30.9%\n",
      "349    ceiling                     141      30.9%\n",
      "350    suit                        141      30.9%\n",
      "351    game                        141      30.9%\n",
      "352    elephants                   141      30.9%\n",
      "353    railing                     141      30.9%\n",
      "354    court                       140      30.6%\n",
      "355    pillow                      140      30.6%\n",
      "356    wheels                      140      30.6%\n",
      "357    couch                       140      30.6%\n",
      "358    mouse                       140      30.6%\n",
      "359    tank                        140      30.6%\n",
      "360    hands                       139      30.4%\n",
      "361    leaf                        139      30.4%\n",
      "362    t                           139      30.4%\n",
      "363    tire                        138      30.2%\n",
      "364    chain                       138      30.2%\n",
      "365    jet                         138      30.2%\n",
      "366    watch                       138      30.2%\n",
      "367    eyes                        137      30.0%\n",
      "368    tracks                      136      29.8%\n",
      "369    lady                        136      29.8%\n",
      "370    engine                      136      29.8%\n",
      "371    giraffes                    136      29.8%\n",
      "372    patch                       135      29.5%\n",
      "373    bread                       133      29.1%\n",
      "374    blanket                     133      29.1%\n",
      "375    donut                       133      29.1%\n",
      "376    sandwich                    132      28.9%\n",
      "377    towel                       132      28.9%\n",
      "378    fur                         132      28.9%\n",
      "379    mouth                       131      28.7%\n",
      "380    tag                         131      28.7%\n",
      "381    glove                       129      28.2%\n",
      "382    tv                          129      28.2%\n",
      "383    keyboard                    128      28.0%\n",
      "384    buildings                   128      28.0%\n",
      "385    stove                       128      28.0%\n",
      "386    cows                        128      28.0%\n",
      "387    these                       127      27.8%\n",
      "388    rocks                       127      27.8%\n",
      "389    spot                        126      27.6%\n",
      "390    guy                         126      27.6%\n",
      "391    bush                        126      27.6%\n",
      "392    van                         126      27.6%\n",
      "393    ocean                       125      27.4%\n",
      "394    wave                        125      27.4%\n",
      "395    teddy                       125      27.4%\n",
      "396    trash                       125      27.4%\n",
      "397    skis                        125      27.4%\n",
      "398    wing                        124      27.1%\n",
      "399    sleeve                      124      27.1%\n",
      "400    men                         121      26.5%\n",
      "401    ears                        121      26.5%\n",
      "402    zebras                      121      26.5%\n",
      "403    curtain                     121      26.5%\n",
      "404    bananas                     120      26.3%\n",
      "405    collar                      120      26.3%\n",
      "406    backpack                    120      26.3%\n",
      "407    sweater                     120      26.3%\n",
      "408    spoon                       119      26.0%\n",
      "409    there                       118      25.8%\n",
      "410    cheese                      118      25.8%\n",
      "411    view                        118      25.8%\n",
      "412    hill                        117      25.6%\n",
      "413    writing                     117      25.6%\n",
      "414    knife                       117      25.6%\n",
      "415    surfer                      117      25.6%\n",
      "416    lid                         117      25.6%\n",
      "417    fork                        116      25.4%\n",
      "418    skier                       116      25.4%\n",
      "419    pavement                    116      25.4%\n",
      "420    frisbee                     115      25.2%\n",
      "421    sunglasses                  115      25.2%\n",
      "422    bat                         114      24.9%\n",
      "423    windshield                  114      24.9%\n",
      "424    scene                       114      24.9%\n",
      "425    branches                    114      24.9%\n",
      "426    numbers                     114      24.9%\n",
      "427    row                         113      24.7%\n",
      "428    branch                      113      24.7%\n",
      "429    letters                     112      24.5%\n",
      "430    growing                     112      24.5%\n",
      "431    distance                    111      24.3%\n",
      "432    jeans                       111      24.3%\n",
      "433    remote                      111      24.3%\n",
      "434    cell                        109      23.9%\n",
      "435    hydrant                     108      23.6%\n",
      "436    broccoli                    107      23.4%\n",
      "437    bushes                      107      23.4%\n",
      "438    pair                        106      23.2%\n",
      "439    wrist                       106      23.2%\n",
      "440    waves                       105      23.0%\n",
      "441    slice                       105      23.0%\n",
      "442    lot                         105      23.0%\n",
      "443    sauce                       105      23.0%\n",
      "444    racket                      104      22.8%\n",
      "445    blonde                      104      22.8%\n",
      "446    monitor                     104      22.8%\n",
      "447    clouds                      100      21.9%\n",
      "448    napkin                       99      21.7%\n",
      "449    mountains                    96      21.0%\n",
      "450    letter                       93      20.4%\n",
      "451    cloud                        92      20.1%\n",
      "452    pile                         92      20.1%\n",
      "453    group                        90      19.7%\n",
      "454    lettering                    88      19.3%\n",
      "455    license                      80      17.5%\n",
      "456    cloudy                       76      16.6%\n",
      "457    taken                        76      16.6%\n",
      "458    bunch                        67      14.7%\n",
      "\n",
      "================================================================================\n",
      "Degree range: 67 - 457\n"
     ]
    }
   ],
   "source": [
    "# NOTE Edges per word\n",
    "print(\"=\" * 80)\n",
    "print(\"EDGES PER NODE (DEGREE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "graph = vg_network_data['graph']\n",
    "\n",
    "# Get degree for each node\n",
    "degrees = dict(graph.degree())\n",
    "\n",
    "# Sort by degree (descending)\n",
    "sorted_degrees = sorted(degrees.items(), key=lambda x: -x[1])\n",
    "\n",
    "print(f\"Total nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"Total edges: {graph.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(degrees.values()) / len(degrees):.2f}\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Rank':<6} {'Word':<20} {'Degree':>10} {'% of Max':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "max_degree = sorted_degrees[0][1] if sorted_degrees else 1\n",
    "for rank, (word, degree) in enumerate(sorted_degrees, 1):\n",
    "    pct = (degree / max_degree) * 100\n",
    "    print(f\"{rank:<6} {word:<20} {degree:>10} {pct:>9.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"Degree range: {min(degrees.values())} - {max(degrees.values())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d080008",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff86d7b",
   "metadata": {},
   "source": [
    "# APPENDING MISSING WORDS USING NEW CORPUS STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f4218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CIFAR-100 COVERAGE IN ORIGINAL VG NETWORK\n",
      "================================================================================\n",
      "VG network vocabulary: 458 words\n",
      "CIFAR-100 classes: 100 words\n",
      "\n",
      "✅ CIFAR-100 words FOUND in VG network: 32/100\n",
      "❌ CIFAR-100 words MISSING from VG network: 68/100\n",
      "\n",
      "WORDS MISSING FROM NETWORK: ['aquarium_fish', 'beaver', 'bee', 'beetle', 'butterfly', 'camel', 'castle', 'caterpillar', 'cattle', 'chimpanzee', 'cockroach', 'crab', 'crocodile', 'dinosaur', 'dolphin', 'flatfish', 'forest', 'fox', 'hamster', 'kangaroo', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'maple_tree', 'mushroom', 'oak_tree', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'telephone', 'television', 'tiger', 'tractor', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'worm']\n",
      "\n",
      "================================================================================\n",
      "WORDS FOUND IN NETWORK:\n",
      "================================================================================\n",
      "   1. apple                (freq: 8,852)\n",
      "   2. baby                 (freq: 14,523)\n",
      "   3. bear                 (freq: 43,395)\n",
      "   4. bed                  (freq: 32,516)\n",
      "   5. bicycle              (freq: 11,238)\n",
      "   6. bottle               (freq: 25,051)\n",
      "   7. bowl                 (freq: 26,525)\n",
      "   8. boy                  (freq: 45,489)\n",
      "   9. bridge               (freq: 9,680)\n",
      "  10. bus                  (freq: 64,424)\n",
      "  11. can                  (freq: 12,865)\n",
      "  12. chair                (freq: 42,202)\n",
      "  13. clock                (freq: 48,573)\n",
      "  14. cloud                (freq: 9,561)\n",
      "  15. couch                (freq: 12,503)\n",
      "  16. cup                  (freq: 19,129)\n",
      "  17. elephant             (freq: 41,969)\n",
      "  18. girl                 (freq: 40,839)\n",
      "  19. house                (freq: 21,186)\n",
      "  20. keyboard             (freq: 13,922)\n",
      "  21. lamp                 (freq: 20,193)\n",
      "  22. man                  (freq: 317,885)\n",
      "  23. motorcycle           (freq: 29,633)\n",
      "  24. mountain             (freq: 13,745)\n",
      "  25. mouse                (freq: 8,793)\n",
      "  26. orange               (freq: 67,542)\n",
      "  27. plate                (freq: 79,913)\n",
      "  28. road                 (freq: 56,377)\n",
      "  29. table                (freq: 114,804)\n",
      "  30. tank                 (freq: 7,281)\n",
      "  31. train                (freq: 100,584)\n",
      "  32. woman                (freq: 138,752)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: Need to add 68 missing words to the network\n"
     ]
    }
   ],
   "source": [
    "# NOTE Checking Missing Words\n",
    "# Check which CIFAR-100 words are missing from the original VG network\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# Get CIFAR-100 class names\n",
    "cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True)\n",
    "cifar_words = set(cifar100.classes)\n",
    "\n",
    "# Get words in the original VG network\n",
    "vg_vocab = set(vg_network_data['nodes'])\n",
    "\n",
    "# Find missing and found words\n",
    "cifar_found = sorted(cifar_words & vg_vocab)\n",
    "cifar_missing = sorted(cifar_words - vg_vocab)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CIFAR-100 COVERAGE IN ORIGINAL VG NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"VG network vocabulary: {len(vg_vocab)} words\")\n",
    "print(f\"CIFAR-100 classes: {len(cifar_words)} words\")\n",
    "print()\n",
    "print(f\"✅ CIFAR-100 words FOUND in VG network: {len(cifar_found)}/100\")\n",
    "print(f\"❌ CIFAR-100 words MISSING from VG network: {len(cifar_missing)}/100\")\n",
    "\n",
    "print()\n",
    "print(f\"WORDS MISSING FROM NETWORK: {cifar_missing}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"WORDS FOUND IN NETWORK:\")\n",
    "print(\"=\" * 80)\n",
    "for i, word in enumerate(cifar_found, 1):\n",
    "    freq = vg_network_data['token_counts'].get(word, 0)\n",
    "    print(f\"  {i:2d}. {word:<20} (freq: {freq:,})\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"SUMMARY: Need to add {len(cifar_missing)} missing words to the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ccd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORPUS SIZE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Metric                       CIFAR-100 Descriptions                   VG Text\n",
      "--------------------------------------------------------------------------------\n",
      "Characters                               48,865,681               154,198,751\n",
      "Words (whitespace)                        8,905,223                32,937,644\n",
      "\n",
      "CIFAR corpus is 31.69% the size of VG corpus (by characters)\n",
      "CIFAR corpus is 27.04% the size of VG corpus (by words)\n"
     ]
    }
   ],
   "source": [
    "# NOTE comparing corpus sizes\n",
    "# Compare word counts between cifar100_word_descriptions.txt and vg_text.txt\n",
    "\n",
    "# Read both files\n",
    "with open('cifar100_word_descriptions.txt', 'r', encoding='utf-8') as f:\n",
    "    cifar_text = f.read()\n",
    "\n",
    "with open('vg_text.txt', 'r', encoding='utf-8') as f:\n",
    "    vg_text = f.read()\n",
    "\n",
    "# Count words (simple split by whitespace)\n",
    "cifar_words_count = len(cifar_text.split())\n",
    "vg_words_count = len(vg_text.split())\n",
    "\n",
    "# Character counts\n",
    "cifar_chars = len(cifar_text)\n",
    "vg_chars = len(vg_text)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORPUS SIZE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"{'Metric':<25} {'CIFAR-100 Descriptions':>25} {'VG Text':>25}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Characters':<25} {cifar_chars:>25,} {vg_chars:>25,}\")\n",
    "print(f\"{'Words (whitespace)':<25} {cifar_words_count:>25,} {vg_words_count:>25,}\")\n",
    "print()\n",
    "print(f\"CIFAR corpus is {cifar_chars / vg_chars * 100:.2f}% the size of VG corpus (by characters)\")\n",
    "print(f\"CIFAR corpus is {cifar_words_count / vg_words_count * 100:.2f}% the size of VG corpus (by words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e29789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing CIFAR-100 words: 68\n",
      "Existing network: 458 nodes, 50,127 edges\n",
      "\n",
      "Tokenizing corpora...\n",
      "  VG tokens: 33,662,592\n",
      "  CIFAR tokens: 9,262,627\n",
      "\n",
      "Computing adjacencies...\n",
      "  VG adjacency pairs: 934,725\n",
      "  CIFAR adjacency pairs: 223,471\n",
      "\n",
      "================================================================================\n",
      "FINDING CO-OCCURRENCES FOR MISSING WORDS\n",
      "================================================================================\n",
      "\n",
      "Word                   # Connections Top 5 Co-occurring Words                          \n",
      "------------------------------------------------------------------------------------------\n",
      "aquarium_fish                     83 .(3728), with(864), of(644), in(604), colorful(488)\n",
      "beaver                            84 .(3561), a(3229), the(1117), in(432), fur(300)    \n",
      "bee                               87 .(4236), a(1896), in(602), little(520), the(446)  \n",
      "beetle                            85 .(3165), a(1811), on(1576), in(1134), with(853)   \n",
      "butterfly                         98 a(4521), .(4012), the(882), in(718), on(519)      \n",
      "camel                            107 .(4535), a(2958), the(925), in(416), back(402)    \n",
      "castle                           134 .(6717), the(3274), a(1324), windows(444), stone(384)\n",
      "caterpillar                       78 .(3152), on(1419), green(1237), in(858), eating(808)\n",
      "cattle                           123 .(5064), of(1456), with(1175), in(1002), the(691) \n",
      "chimpanzee                        74 .(5099), a(2366), in(460), the(447), young(296)   \n",
      "cockroach                         89 .(3594), a(1811), on(1625), in(1301), the(831)    \n",
      "crab                             130 .(3469), a(2241), blue(578), on(551), in(472)     \n",
      "crocodile                         81 .(6124), a(1605), the(834), in(221), this(200)    \n",
      "dinosaur                         114 .(5110), a(1838), in(525), standing(388), the(294)\n",
      "dolphin                           94 .(5502), a(2479), the(521), and(340), in(237)     \n",
      "flatfish                          90 .(6036), a(1432), with(1016), in(896), of(892)    \n",
      "forest                           138 .(24701), the(10563), floor(8737), a(2848), in(2137)\n",
      "fox                               92 .(5141), a(2301), red(1071), in(705), with(518)   \n",
      "hamster                           75 .(3961), little(1505), in(747), a(489), on(447)   \n",
      "kangaroo                          86 .(4230), a(2950), in(972), with(467), the(421)    \n",
      "lawn_mower                        96 .(5964), the(3568), a(692), with(540), old(520)   \n",
      "leopard                           94 .(5944), a(2651), the(1027), in(429), fur(398)    \n",
      "lion                             112 .(3474), a(2612), the(1252), in(641), with(363)   \n",
      "lizard                            93 .(4219), a(1434), the(707), small(625), with(524) \n",
      "lobster                          111 .(4679), a(1077), in(679), on(580), with(484)     \n",
      "maple_tree                        79 .(7580), the(1772), a(1036), leaves(836), branches(760)\n",
      "mushroom                         124 .(4936), a(1788), cap(841), on(701), in(674)      \n",
      "oak_tree                          71 .(6784), the(1996), an(792), branches(732), leaves(664)\n",
      "orchid                            84 .(3081), an(1855), the(1028), in(642), white(621) \n",
      "otter                             80 .(7604), an(1195), fur(408), in(281), playing(276)\n",
      "palm_tree                         84 .(7120), a(1124), the(812), in(684), with(604)    \n",
      "pear                             102 .(3847), a(1574), the(692), on(638), green(600)   \n",
      "pickup_truck                      99 .(5800), with(1776), old(1548), in(864), parked(788)\n",
      "pine_tree                         77 .(7336), the(1488), a(1468), in(692), on(596)     \n",
      "plain                            230 .(5074), a(3903), the(1768), white(1708), and(461)\n",
      "poppy                             88 .(3652), red(2015), a(1315), the(527), flowers(510)\n",
      "porcupine                         81 .(4641), a(2409), this(844), in(768), the(721)    \n",
      "possum                            90 .(3168), a(3136), the(540), little(496), in(480)  \n",
      "rabbit                           106 .(4054), in(1195), little(1193), a(895), with(739)\n",
      "raccoon                           81 .(4881), a(2119), little(716), in(492), on(284)   \n",
      "ray                              102 .(7417), of(3642), a(1272), the(412), on(340)     \n",
      "rocket                           112 .(3933), a(2816), the(1122), in(431), with(392)   \n",
      "rose                             141 .(5115), a(2221), in(1349), red(1029), pink(600)  \n",
      "sea                              151 .(12119), the(5264), a(835), of(820), at(615)     \n",
      "seal                             108 .(5528), a(1729), on(574), fur(380), of(365)      \n",
      "shark                             97 .(5897), a(2161), the(550), in(410), white(368)   \n",
      "shrew                             75 .(5516), a(2072), the(840), in(760), small(636)   \n",
      "skunk                             87 .(4002), a(1251), in(1148), little(872), with(804)\n",
      "skyscraper                       110 .(5691), the(1785), a(977), in(655), with(618)    \n",
      "snail                             83 .(3785), a(1611), on(1433), the(1004), in(904)    \n",
      "snake                             84 .(6793), a(2220), in(630), the(527), body(480)    \n",
      "spider                            88 .(3456), a(2195), the(1169), on(973), in(672)     \n",
      "squirrel                          95 .(5010), a(1908), grey(770), the(681), in(599)    \n",
      "streetcar                         98 .(7025), with(1022), in(777), old(620), on(592)   \n",
      "sunflower                        105 .(4778), a(1903), in(666), yellow(476), field(468)\n",
      "sweet_pepper                      61 .(3684), and(1768), on(404), with(392), green(300)\n",
      "telephone                        148 .(5965), on(2158), old(2008), pole(1462), with(1421)\n",
      "television                       138 .(9638), screen(2085), on(1988), old(1978), the(1926)\n",
      "tiger                            109 .(4598), a(3593), the(1309), in(555), eyes(468)   \n",
      "tractor                          122 .(5323), old(1429), a(1348), with(1325), in(1071) \n",
      "trout                             90 .(5801), a(1472), with(849), in(841), on(537)     \n",
      "tulip                             97 .(3224), a(1771), in(618), the(504), red(461)     \n",
      "turtle                           119 .(3011), on(984), a(922), in(747), green(555)     \n",
      "wardrobe                         122 .(6730), with(1153), in(1104), the(1095), old(920)\n",
      "whale                             93 .(4585), a(1819), the(519), blue(482), in(317)    \n",
      "willow_tree                       74 .(5888), a(1968), the(1880), leaves(600), branches(560)\n",
      "wolf                              84 .(4849), a(2602), the(1027), gray(904), in(546)   \n",
      "worm                              73 .(1954), in(1616), a(1454), small(1112), on(938)  \n",
      "\n",
      "⚠️  Words with NO co-occurrences in either corpus: 0\n"
     ]
    }
   ],
   "source": [
    "# NOTE Appending missing CIFAR-100 words based on co-occurrences from BOTH corpora\n",
    "# ============================================================================\n",
    "# STRATEGY: Use actual co-occurrence data from vg_text.txt AND cifar100_word_descriptions.txt\n",
    "# to find which existing network words should connect to missing CIFAR-100 words.\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from collections import Counter\n",
    "from lab2 import tokenize_text, get_text_adjacencies\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Identify missing CIFAR-100 words\n",
    "# ============================================================================\n",
    "cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True)\n",
    "cifar_words = set(cifar100.classes)\n",
    "existing_vocab = set(vg_network_data['nodes'])\n",
    "missing_words = sorted(cifar_words - existing_vocab)\n",
    "\n",
    "print(f\"Missing CIFAR-100 words: {len(missing_words)}\")\n",
    "print(f\"Existing network: {len(existing_vocab)} nodes, {vg_network_data['graph'].number_of_edges():,} edges\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Tokenize both corpora\n",
    "# ============================================================================\n",
    "print(\"\\nTokenizing corpora...\")\n",
    "\n",
    "# VG text\n",
    "with open('vg_text.txt', 'r', encoding='utf-8') as f:\n",
    "    vg_text_content = f.read()\n",
    "vg_tokens = tokenize_text(vg_text_content)\n",
    "print(f\"  VG tokens: {len(vg_tokens):,}\")\n",
    "\n",
    "# CIFAR descriptions\n",
    "with open('cifar100_word_descriptions.txt', 'r', encoding='utf-8') as f:\n",
    "    cifar_text_content = f.read()\n",
    "cifar_tokens = tokenize_text(cifar_text_content)\n",
    "print(f\"  CIFAR tokens: {len(cifar_tokens):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Get adjacencies from both corpora\n",
    "# ============================================================================\n",
    "print(\"\\nComputing adjacencies...\")\n",
    "vg_adjacencies = get_text_adjacencies(vg_tokens)\n",
    "cifar_adjacencies = get_text_adjacencies(cifar_tokens)\n",
    "print(f\"  VG adjacency pairs: {len(vg_adjacencies):,}\")\n",
    "print(f\"  CIFAR adjacency pairs: {len(cifar_adjacencies):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. For each missing word, find co-occurring words that ARE in the network\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINDING CO-OCCURRENCES FOR MISSING WORDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine adjacencies from both corpora\n",
    "combined_adjacencies = Counter()\n",
    "combined_adjacencies.update(vg_adjacencies)\n",
    "combined_adjacencies.update(cifar_adjacencies)\n",
    "\n",
    "# For each missing word, find which existing network words it co-occurs with\n",
    "missing_word_connections = {}\n",
    "\n",
    "for word in missing_words:\n",
    "    connections = Counter()\n",
    "    \n",
    "    # Check all adjacency pairs involving this word\n",
    "    for (w1, w2), count in combined_adjacencies.items():\n",
    "        if w1 == word and w2 in existing_vocab:\n",
    "            connections[w2] += count\n",
    "        elif w2 == word and w1 in existing_vocab:\n",
    "            connections[w1] += count\n",
    "    \n",
    "    missing_word_connections[word] = connections\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'Word':<20} {'# Connections':>15} {'Top 5 Co-occurring Words':<50}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for word in missing_words:\n",
    "    conns = missing_word_connections[word]\n",
    "    n_conns = len(conns)\n",
    "    top5 = conns.most_common(5)\n",
    "    top5_str = ', '.join([f\"{w}({c})\" for w, c in top5]) if top5 else \"(none found)\"\n",
    "    print(f\"{word:<20} {n_conns:>15} {top5_str:<50}\")\n",
    "\n",
    "# Count words with no connections\n",
    "no_conns = [w for w in missing_words if len(missing_word_connections[w]) == 0]\n",
    "print(f\"\\n⚠️  Words with NO co-occurrences in either corpus: {len(no_conns)}\")\n",
    "if no_conns:\n",
    "    print(f\"   {no_conns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf691c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing network distance stats:\n",
      "  Min: 1.0000, Median: 970325.5000, Max: 970325.5000\n",
      "\n",
      "Corpus scale factor: 3.63x (VG is 3.6x larger)\n",
      "Median VG adjacency count: 6\n",
      "\n",
      "Adding 68 missing words to network...\n",
      "✓ Added 68 nodes and 6790 edges\n",
      "\n",
      "================================================================================\n",
      "AUGMENTED NETWORK SUMMARY\n",
      "================================================================================\n",
      "Original network: 458 nodes, 50,127 edges\n",
      "Augmented network: 526 nodes, 56,917 edges\n",
      "New edges added: 6790\n",
      "\n",
      "CIFAR-100 coverage: 100/100\n",
      "✅ ALL 100 CIFAR-100 WORDS ARE IN THE NETWORK!\n",
      "\n",
      "✓ Saved augmented_network to 'augmented_network.pkl'\n"
     ]
    }
   ],
   "source": [
    "# NOTE Using the co-occurrence data to add edges and compute distances\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Copy existing network components\n",
    "# ============================================================================\n",
    "graph = vg_network_data['graph'].copy()\n",
    "nodes = list(vg_network_data['nodes'])\n",
    "distance_matrix = vg_network_data['distance_matrix'].copy()\n",
    "token_counts = dict(vg_network_data['token_counts'])\n",
    "\n",
    "# Get existing distance statistics (for scaling new distances)\n",
    "existing_distances = distance_matrix[distance_matrix > 0]\n",
    "median_distance = np.median(existing_distances)\n",
    "min_distance = np.min(existing_distances)\n",
    "max_distance = np.max(existing_distances)\n",
    "\n",
    "print(\"Existing network distance stats:\")\n",
    "print(f\"  Min: {min_distance:.4f}, Median: {median_distance:.4f}, Max: {max_distance:.4f}\")\n",
    "\n",
    "# Node index mapping\n",
    "node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Expand distance matrix to accommodate new words\n",
    "# ============================================================================\n",
    "n_original = len(nodes)\n",
    "n_new = len(missing_words)\n",
    "n_total = n_original + n_new\n",
    "\n",
    "# New matrix: fill with max_distance (unconnected pairs)\n",
    "new_distance_matrix = np.full((n_total, n_total), fill_value=max_distance)\n",
    "new_distance_matrix[:n_original, :n_original] = distance_matrix\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Add each missing word with edges based on co-occurrence counts\n",
    "# ============================================================================\n",
    "# The CIFAR corpus is ~27% the size of VG corpus, so we need to scale counts\n",
    "# to make distances comparable to the VG network distances\n",
    "\n",
    "# Get VG corpus statistics for scaling\n",
    "vg_total_tokens = len(vg_tokens)\n",
    "cifar_total_tokens = len(cifar_tokens)\n",
    "corpus_scale_factor = vg_total_tokens / cifar_total_tokens\n",
    "\n",
    "print(f\"\\nCorpus scale factor: {corpus_scale_factor:.2f}x (VG is {corpus_scale_factor:.1f}x larger)\")\n",
    "\n",
    "# Get a reference for what \"typical\" adjacency counts look like in VG\n",
    "vg_adj_counts = list(vg_network_data['adjacency_counts'].values())\n",
    "median_vg_adj = np.median(vg_adj_counts)\n",
    "print(f\"Median VG adjacency count: {median_vg_adj:.0f}\")\n",
    "\n",
    "print(f\"\\nAdding {len(missing_words)} missing words to network...\")\n",
    "\n",
    "edges_added_total = 0\n",
    "for word in missing_words:\n",
    "    # Add node\n",
    "    graph.add_node(word)\n",
    "    new_idx = len(nodes)\n",
    "    nodes.append(word)\n",
    "    node_to_idx[word] = new_idx\n",
    "    \n",
    "    # Get this word's connections\n",
    "    conns = missing_word_connections[word]\n",
    "    \n",
    "    # Set token count based on total co-occurrence frequency\n",
    "    # (sum of all connections, scaled by corpus ratio)\n",
    "    raw_count = sum(conns.values())\n",
    "    scaled_count = int(raw_count * corpus_scale_factor) if raw_count > 0 else 1000\n",
    "    token_counts[word] = max(scaled_count, 1000)  # Minimum count to avoid filtering\n",
    "    \n",
    "    # Add edges and compute distances\n",
    "    for connected_word, co_count in conns.items():\n",
    "        connected_idx = node_to_idx[connected_word]\n",
    "        \n",
    "        # Add edge to graph\n",
    "        graph.add_edge(word, connected_word)\n",
    "        edges_added_total += 1\n",
    "        \n",
    "        # Compute distance: inversely proportional to (scaled) co-occurrence count\n",
    "        # Higher co-occurrence = lower distance (closer relationship)\n",
    "        scaled_co_count = co_count * corpus_scale_factor\n",
    "        \n",
    "        # Use inverse relationship similar to how VG distances are computed\n",
    "        # distance = max_count / count (so higher count = lower distance)\n",
    "        distance = median_vg_adj / max(scaled_co_count, 1)\n",
    "        distance = np.clip(distance, min_distance, max_distance)\n",
    "        \n",
    "        new_distance_matrix[new_idx, connected_idx] = distance\n",
    "        new_distance_matrix[connected_idx, new_idx] = distance\n",
    "    \n",
    "    # Self-distance = 0\n",
    "    new_distance_matrix[new_idx, new_idx] = 0.0\n",
    "\n",
    "print(f\"✓ Added {len(missing_words)} nodes and {edges_added_total} edges\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Create augmented network dictionary\n",
    "# ============================================================================\n",
    "augmented_network = {\n",
    "    'graph': graph,\n",
    "    'nodes': nodes,\n",
    "    'distance_matrix': new_distance_matrix,\n",
    "    'token_counts': token_counts,\n",
    "    'count_matrix': None,\n",
    "    'adjacency_counts': None,  # Would need to be recomputed if needed\n",
    "    'rare_tokens': vg_network_data['rare_tokens'],\n",
    "    'original_tokens': vg_network_data['original_tokens'],\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Verify coverage\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AUGMENTED NETWORK SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_vocab = set(augmented_network['nodes'])\n",
    "cifar_found = cifar_words & final_vocab\n",
    "cifar_missing_final = sorted(cifar_words - final_vocab)\n",
    "\n",
    "print(f\"Original network: {n_original} nodes, {vg_network_data['graph'].number_of_edges():,} edges\")\n",
    "print(f\"Augmented network: {len(final_vocab)} nodes, {graph.number_of_edges():,} edges\")\n",
    "print(f\"New edges added: {edges_added_total}\")\n",
    "print()\n",
    "print(f\"CIFAR-100 coverage: {len(cifar_found)}/100\")\n",
    "\n",
    "if cifar_missing_final:\n",
    "    print(f\"❌ Still missing: {cifar_missing_final}\")\n",
    "else:\n",
    "    print(\"✅ ALL 100 CIFAR-100 WORDS ARE IN THE NETWORK!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Save for use in other notebooks\n",
    "# ============================================================================\n",
    "import pickle\n",
    "with open('augmented_network.pkl', 'wb') as f:\n",
    "    pickle.dump(augmented_network, f)\n",
    "print(\"\\n✓ Saved augmented_network to 'augmented_network.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f3f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EDGES PER NODE (DEGREE)\n",
      "================================================================================\n",
      "Total nodes: 526\n",
      "Total edges: 56917\n",
      "Average degree: 216.41\n",
      "\n",
      "Rank   Word                     Degree   % of Max\n",
      "--------------------------------------------------\n",
      "1      .                           525     100.0%\n",
      "2      is                          525     100.0%\n",
      "3      the                         524      99.8%\n",
      "4      on                          523      99.6%\n",
      "5      of                          523      99.6%\n",
      "6      in                          523      99.6%\n",
      "7      and                         523      99.6%\n",
      "8      a                           522      99.4%\n",
      "9      with                        522      99.4%\n",
      "10     ,                           521      99.2%\n",
      "11     to                          518      98.7%\n",
      "12     white                       517      98.5%\n",
      "13     at                          508      96.8%\n",
      "14     black                       505      96.2%\n",
      "15     by                          504      96.0%\n",
      "16     for                         503      95.8%\n",
      "17     small                       501      95.4%\n",
      "18     has                         499      95.0%\n",
      "19     brown                       498      94.9%\n",
      "20     red                         497      94.7%\n",
      "21     from                        496      94.5%\n",
      "22     green                       494      94.1%\n",
      "23     yellow                      490      93.3%\n",
      "24     blue                        489      93.1%\n",
      "25     are                         489      93.1%\n",
      "26     large                       487      92.8%\n",
      "27     behind                      486      92.6%\n",
      "28     over                        485      92.4%\n",
      "29     under                       481      91.6%\n",
      "30     up                          477      90.9%\n",
      "31     orange                      476      90.7%\n",
      "32     dark                        475      90.5%\n",
      "33     colored                     475      90.5%\n",
      "34     grey                        471      89.7%\n",
      "35     near                        471      89.7%\n",
      "36     around                      466      88.8%\n",
      "37     gray                        464      88.4%\n",
      "38     <RARE>                      457      87.0%\n",
      "39     this                        452      86.1%\n",
      "40     above                       449      85.5%\n",
      "41     that                        446      85.0%\n",
      "42     holding                     445      84.8%\n",
      "43     pink                        440      83.8%\n",
      "44     sitting                     433      82.5%\n",
      "45     inside                      433      82.5%\n",
      "46     two                         430      81.9%\n",
      "47     big                         428      81.5%\n",
      "48     one                         421      80.2%\n",
      "49     between                     419      79.8%\n",
      "50     looking                     418      79.6%\n",
      "51     against                     416      79.2%\n",
      "52     next                        415      79.0%\n",
      "53     silver                      412      78.5%\n",
      "54     standing                    411      78.3%\n",
      "55     back                        411      78.3%\n",
      "56     long                        409      77.9%\n",
      "57     beside                      408      77.7%\n",
      "58     color                       407      77.5%\n",
      "59     covered                     407      77.5%\n",
      "60     side                        406      77.3%\n",
      "61     out                         406      77.3%\n",
      "62     off                         402      76.6%\n",
      "63     light                       397      75.6%\n",
      "64     tan                         397      75.6%\n",
      "65     left                        395      75.2%\n",
      "66     top                         392      74.7%\n",
      "67     right                       392      74.7%\n",
      "68     purple                      392      74.7%\n",
      "69     hanging                     391      74.5%\n",
      "70     metal                       390      74.3%\n",
      "71     outside                     389      74.1%\n",
      "72     through                     385      73.3%\n",
      "73     colorful                    385      73.3%\n",
      "74     painted                     379      72.2%\n",
      "75     down                        376      71.6%\n",
      "76     little                      376      71.6%\n",
      "77     old                         375      71.4%\n",
      "78     wooden                      369      70.3%\n",
      "79     tall                        367      69.9%\n",
      "80     wearing                     366      69.7%\n",
      "81     three                       358      68.2%\n",
      "82     gold                        357      68.0%\n",
      "83     visible                     357      68.0%\n",
      "84     round                       356      67.8%\n",
      "85     plastic                     352      67.0%\n",
      "86     front                       349      66.5%\n",
      "87     open                        349      66.5%\n",
      "88     into                        345      65.7%\n",
      "89     made                        340      64.8%\n",
      "90     man                         328      62.5%\n",
      "91     sign                        326      62.1%\n",
      "92     being                       325      61.9%\n",
      "93     have                        319      60.8%\n",
      "94     striped                     317      60.4%\n",
      "95     face                        316      60.2%\n",
      "96     some                        316      60.2%\n",
      "97     can                         314      59.8%\n",
      "98     beige                       313      59.6%\n",
      "99     head                        307      58.5%\n",
      "100    his                         306      58.3%\n",
      "101    wood                        302      57.5%\n",
      "102    area                        302      57.5%\n",
      "103    along                       301      57.3%\n",
      "104    short                       300      57.1%\n",
      "105    an                          299      57.0%\n",
      "106    wall                        297      56.6%\n",
      "107    glass                       297      56.6%\n",
      "108    stand                       297      56.6%\n",
      "109    clear                       292      55.6%\n",
      "110    four                        290      55.2%\n",
      "111    square                      290      55.2%\n",
      "112    full                        290      55.2%\n",
      "113    no                          289      55.0%\n",
      "114    water                       288      54.9%\n",
      "115    cover                       288      54.9%\n",
      "116    other                       287      54.7%\n",
      "117    bottom                      285      54.3%\n",
      "118    design                      284      54.1%\n",
      "119    laying                      282      53.7%\n",
      "120    empty                       277      52.8%\n",
      "121    walking                     270      51.4%\n",
      "122    table                       268      51.0%\n",
      "123    attached                    268      51.0%\n",
      "124    eating                      266      50.7%\n",
      "125    set                         265      50.5%\n",
      "126    carrying                    263      50.1%\n",
      "127    train                       262      49.9%\n",
      "128    middle                      262      49.9%\n",
      "129    window                      260      49.5%\n",
      "130    dog                         260      49.5%\n",
      "131    word                        260      49.5%\n",
      "132    box                         259      49.3%\n",
      "133    her                         259      49.3%\n",
      "134    watching                    259      49.3%\n",
      "135    car                         258      49.1%\n",
      "136    line                        258      49.1%\n",
      "137    woman                       257      49.0%\n",
      "138    hand                        257      49.0%\n",
      "139    wet                         257      49.0%\n",
      "140    display                     255      48.6%\n",
      "141    tree                        254      48.4%\n",
      "142    baby                        254      48.4%\n",
      "143    bright                      253      48.2%\n",
      "144    shadow                      252      48.0%\n",
      "145    playing                     252      48.0%\n",
      "146    board                       249      47.4%\n",
      "147    person                      245      46.7%\n",
      "148    stone                       245      46.7%\n",
      "149    building                    244      46.5%\n",
      "150    worn                        244      46.5%\n",
      "151    many                        243      46.3%\n",
      "152    bear                        242      46.1%\n",
      "153    paper                       239      45.5%\n",
      "154    food                        238      45.3%\n",
      "155    riding                      238      45.3%\n",
      "156    several                     238      45.3%\n",
      "157    picture                     236      45.0%\n",
      "158    logo                        236      45.0%\n",
      "159    bus                         235      44.8%\n",
      "160    door                        234      44.6%\n",
      "161    street                      233      44.4%\n",
      "162    cat                         233      44.4%\n",
      "163    concrete                    231      44.0%\n",
      "164    floor                       230      43.8%\n",
      "165    plain                       230      43.8%\n",
      "166    bed                         229      43.6%\n",
      "167    surface                     228      43.4%\n",
      "168    he                          228      43.4%\n",
      "169    bike                        227      43.2%\n",
      "170    grass                       226      43.0%\n",
      "171    chair                       226      43.0%\n",
      "172    man's                       226      43.0%\n",
      "173    plate                       225      42.9%\n",
      "174    pole                        225      42.9%\n",
      "175    background                  225      42.9%\n",
      "176    flying                      225      42.9%\n",
      "177    shirt                       224      42.7%\n",
      "178    bench                       224      42.7%\n",
      "179    cake                        224      42.7%\n",
      "180    body                        223      42.5%\n",
      "181    tail                        222      42.3%\n",
      "182    flower                      221      42.1%\n",
      "183    clock                       220      41.9%\n",
      "184    part                        218      41.5%\n",
      "185    snow                        218      41.5%\n",
      "186    boat                        218      41.5%\n",
      "187    base                        217      41.3%\n",
      "188    lights                      215      41.0%\n",
      "189    truck                       214      40.8%\n",
      "190    ball                        214      40.8%\n",
      "191    elephant                    213      40.6%\n",
      "192    handle                      213      40.6%\n",
      "193    house                       213      40.6%\n",
      "194    closed                      213      40.6%\n",
      "195    tennis                      212      40.4%\n",
      "196    road                        210      40.0%\n",
      "197    horse                       210      40.0%\n",
      "198    ski                         210      40.0%\n",
      "199    frame                       210      40.0%\n",
      "200    woman's                     208      39.6%\n",
      "201    corner                      208      39.6%\n",
      "202    wire                        208      39.6%\n",
      "203    young                       207      39.4%\n",
      "204    motorcycle                  206      39.2%\n",
      "205    rear                        206      39.2%\n",
      "206    seat                        205      39.0%\n",
      "207    bag                         204      38.9%\n",
      "208    brick                       204      38.9%\n",
      "209    piece                       204      38.9%\n",
      "210    fence                       203      38.7%\n",
      "211    girl                        203      38.7%\n",
      "212    edge                        203      38.7%\n",
      "213    animal                      202      38.5%\n",
      "214    cement                      202      38.5%\n",
      "215    ground                      200      38.1%\n",
      "216    hair                        200      38.1%\n",
      "217    reflection                  200      38.1%\n",
      "218    leaves                      198      37.7%\n",
      "219    boy                         198      37.7%\n",
      "220    bird                        198      37.7%\n",
      "221    beach                       198      37.7%\n",
      "222    arm                         198      37.7%\n",
      "223    wears                       198      37.7%\n",
      "224    park                        197      37.5%\n",
      "225    cloth                       197      37.5%\n",
      "226    smiling                     197      37.5%\n",
      "227    statue                      197      37.5%\n",
      "228    lines                       196      37.3%\n",
      "229    foot                        196      37.3%\n",
      "230    hat                         195      37.1%\n",
      "231    number                      194      37.0%\n",
      "232    kite                        193      36.8%\n",
      "233    kitchen                     193      36.8%\n",
      "234    field                       191      36.4%\n",
      "235    post                        191      36.4%\n",
      "236    stuffed                     191      36.4%\n",
      "237    plane                       190      36.2%\n",
      "238    baseball                    190      36.2%\n",
      "239    umbrella                    190      36.2%\n",
      "240    leg                         190      36.2%\n",
      "241    dirt                        190      36.2%\n",
      "242    section                     190      36.2%\n",
      "243    nose                        189      36.0%\n",
      "244    rock                        189      36.0%\n",
      "245    end                         189      36.0%\n",
      "246    lamp                        188      35.8%\n",
      "247    tower                       188      35.8%\n",
      "248    fire                        188      35.8%\n",
      "249    double                      188      35.8%\n",
      "250    plant                       187      35.6%\n",
      "251    legs                        186      35.4%\n",
      "252    giraffe                     185      35.2%\n",
      "253    windows                     185      35.2%\n",
      "254    airplane                    185      35.2%\n",
      "255    feet                        185      35.2%\n",
      "256    pizza                       184      35.0%\n",
      "257    eye                         184      35.0%\n",
      "258    bathroom                    184      35.0%\n",
      "259    toilet                      183      34.9%\n",
      "260    bowl                        183      34.9%\n",
      "261    roof                        183      34.9%\n",
      "262    sky                         182      34.7%\n",
      "263    eyes                        180      34.3%\n",
      "264    parked                      179      34.1%\n",
      "265    skateboard                  179      34.1%\n",
      "266    cap                         179      34.1%\n",
      "267    coat                        179      34.1%\n",
      "268    bicycle                     179      34.1%\n",
      "269    air                         178      33.9%\n",
      "270    cow                         178      33.9%\n",
      "271    wheel                       178      33.9%\n",
      "272    room                        178      33.9%\n",
      "273    child                       178      33.9%\n",
      "274    track                       178      33.9%\n",
      "275    cup                         177      33.7%\n",
      "276    sidewalk                    176      33.5%\n",
      "277    traffic                     176      33.5%\n",
      "278    trees                       175      33.3%\n",
      "279    mirror                      175      33.3%\n",
      "280    container                   175      33.3%\n",
      "281    people                      174      33.1%\n",
      "282    jacket                      174      33.1%\n",
      "283    zebra                       173      33.0%\n",
      "284    ear                         173      33.0%\n",
      "285    phone                       173      33.0%\n",
      "286    stop                        173      33.0%\n",
      "287    flag                        173      33.0%\n",
      "288    neck                        171      32.6%\n",
      "289    parking                     171      32.6%\n",
      "290    banana                      171      32.6%\n",
      "291    city                        171      32.6%\n",
      "292    apple                       171      32.6%\n",
      "293    platform                    170      32.4%\n",
      "294    desk                        169      32.2%\n",
      "295    computer                    169      32.2%\n",
      "296    button                      169      32.2%\n",
      "297    hot                         169      32.2%\n",
      "298    sun                         169      32.2%\n",
      "299    shelf                       168      32.0%\n",
      "300    tray                        168      32.0%\n",
      "301    rail                        168      32.0%\n",
      "302    very                        167      31.8%\n",
      "303    it                          166      31.6%\n",
      "304    shoes                       166      31.6%\n",
      "305    sand                        166      31.6%\n",
      "306    photo                       166      31.6%\n",
      "307    flowers                     165      31.4%\n",
      "308    vehicle                     165      31.4%\n",
      "309    stripe                      165      31.4%\n",
      "310    book                        165      31.4%\n",
      "311    screen                      164      31.2%\n",
      "312    bridge                      164      31.2%\n",
      "313    tie                         163      31.0%\n",
      "314    stripes                     163      31.0%\n",
      "315    rack                        163      31.0%\n",
      "316    tracks                      162      30.9%\n",
      "317    fruit                       162      30.9%\n",
      "318    pants                       161      30.7%\n",
      "319    signs                       161      30.7%\n",
      "320    sheep                       160      30.5%\n",
      "321    trunk                       160      30.5%\n",
      "322    leaf                        159      30.3%\n",
      "323    passenger                   159      30.3%\n",
      "324    luggage                     159      30.3%\n",
      "325    vase                        158      30.1%\n",
      "326    tile                        158      30.1%\n",
      "327    dish                        158      30.1%\n",
      "328    fur                         157      29.9%\n",
      "329    sink                        156      29.7%\n",
      "330    poles                       156      29.7%\n",
      "331    player                      155      29.5%\n",
      "332    laptop                      155      29.5%\n",
      "333    shoe                        155      29.5%\n",
      "334    basket                      155      29.5%\n",
      "335    helmet                      154      29.3%\n",
      "336    mountain                    153      29.1%\n",
      "337    dress                       153      29.1%\n",
      "338    watch                       153      29.1%\n",
      "339    bottle                      151      28.8%\n",
      "340    pot                         151      28.8%\n",
      "341    sea                         151      28.8%\n",
      "342    oven                        150      28.6%\n",
      "343    counter                     149      28.4%\n",
      "344    mouth                       149      28.4%\n",
      "345    wine                        149      28.4%\n",
      "346    surfboard                   148      28.2%\n",
      "347    suit                        148      28.2%\n",
      "348    scene                       148      28.2%\n",
      "349    telephone                   148      28.2%\n",
      "350    glasses                     147      28.0%\n",
      "351    ceiling                     147      28.0%\n",
      "352    wheels                      147      28.0%\n",
      "353    camera                      147      28.0%\n",
      "354    cabinet                     147      28.0%\n",
      "355    horses                      147      28.0%\n",
      "356    coffee                      146      27.8%\n",
      "357    skiing                      145      27.6%\n",
      "358    person's                    145      27.6%\n",
      "359    patch                       144      27.4%\n",
      "360    t                           144      27.4%\n",
      "361    tank                        144      27.4%\n",
      "362    pillow                      143      27.2%\n",
      "363    hands                       143      27.2%\n",
      "364    game                        143      27.2%\n",
      "365    ears                        143      27.2%\n",
      "366    shorts                      142      27.0%\n",
      "367    tire                        142      27.0%\n",
      "368    cars                        142      27.0%\n",
      "369    suitcase                    142      27.0%\n",
      "370    engine                      142      27.0%\n",
      "371    elephants                   142      27.0%\n",
      "372    mouse                       142      27.0%\n",
      "373    couch                       141      26.9%\n",
      "374    railing                     141      26.9%\n",
      "375    rose                        141      26.9%\n",
      "376    court                       140      26.7%\n",
      "377    lady                        138      26.3%\n",
      "378    chain                       138      26.3%\n",
      "379    jet                         138      26.3%\n",
      "380    forest                      138      26.3%\n",
      "381    television                  138      26.3%\n",
      "382    ocean                       137      26.1%\n",
      "383    bread                       137      26.1%\n",
      "384    blanket                     137      26.1%\n",
      "385    sandwich                    136      25.9%\n",
      "386    giraffes                    136      25.9%\n",
      "387    towel                       134      25.5%\n",
      "388    donut                       134      25.5%\n",
      "389    castle                      134      25.5%\n",
      "390    view                        133      25.3%\n",
      "391    rocks                       132      25.1%\n",
      "392    spot                        132      25.1%\n",
      "393    tag                         132      25.1%\n",
      "394    these                       130      24.8%\n",
      "395    wing                        130      24.8%\n",
      "396    buildings                   130      24.8%\n",
      "397    guy                         130      24.8%\n",
      "398    glove                       130      24.8%\n",
      "399    crab                        130      24.8%\n",
      "400    wave                        129      24.6%\n",
      "401    keyboard                    129      24.6%\n",
      "402    tv                          129      24.6%\n",
      "403    stove                       128      24.4%\n",
      "404    bush                        128      24.4%\n",
      "405    cows                        128      24.4%\n",
      "406    teddy                       127      24.2%\n",
      "407    sleeve                      127      24.2%\n",
      "408    growing                     126      24.0%\n",
      "409    van                         126      24.0%\n",
      "410    branches                    126      24.0%\n",
      "411    trash                       125      23.8%\n",
      "412    skis                        125      23.8%\n",
      "413    curtain                     124      23.6%\n",
      "414    mushroom                    124      23.6%\n",
      "415    hill                        123      23.4%\n",
      "416    cattle                      123      23.4%\n",
      "417    branch                      122      23.2%\n",
      "418    tractor                     122      23.2%\n",
      "419    wardrobe                    122      23.2%\n",
      "420    there                       121      23.0%\n",
      "421    men                         121      23.0%\n",
      "422    zebras                      121      23.0%\n",
      "423    cheese                      120      22.9%\n",
      "424    bananas                     120      22.9%\n",
      "425    collar                      120      22.9%\n",
      "426    backpack                    120      22.9%\n",
      "427    sweater                     120      22.9%\n",
      "428    spoon                       119      22.7%\n",
      "429    turtle                      119      22.7%\n",
      "430    distance                    118      22.5%\n",
      "431    knife                       118      22.5%\n",
      "432    surfer                      118      22.5%\n",
      "433    writing                     117      22.3%\n",
      "434    fork                        117      22.3%\n",
      "435    lid                         117      22.3%\n",
      "436    row                         116      22.1%\n",
      "437    skier                       116      22.1%\n",
      "438    pavement                    116      22.1%\n",
      "439    frisbee                     115      21.9%\n",
      "440    sunglasses                  115      21.9%\n",
      "441    windshield                  115      21.9%\n",
      "442    numbers                     115      21.9%\n",
      "443    pair                        114      21.7%\n",
      "444    bat                         114      21.7%\n",
      "445    remote                      114      21.7%\n",
      "446    dinosaur                    114      21.7%\n",
      "447    waves                       113      21.5%\n",
      "448    letters                     112      21.3%\n",
      "449    lion                        112      21.3%\n",
      "450    rocket                      112      21.3%\n",
      "451    jeans                       111      21.1%\n",
      "452    slice                       111      21.1%\n",
      "453    lobster                     111      21.1%\n",
      "454    monitor                     110      21.0%\n",
      "455    skyscraper                  110      21.0%\n",
      "456    cell                        109      20.8%\n",
      "457    broccoli                    109      20.8%\n",
      "458    tiger                       109      20.8%\n",
      "459    hydrant                     108      20.6%\n",
      "460    bushes                      108      20.6%\n",
      "461    seal                        108      20.6%\n",
      "462    sauce                       107      20.4%\n",
      "463    camel                       107      20.4%\n",
      "464    wrist                       106      20.2%\n",
      "465    lot                         106      20.2%\n",
      "466    rabbit                      106      20.2%\n",
      "467    sunflower                   105      20.0%\n",
      "468    racket                      104      19.8%\n",
      "469    blonde                      104      19.8%\n",
      "470    pear                        102      19.4%\n",
      "471    ray                         102      19.4%\n",
      "472    clouds                      101      19.2%\n",
      "473    napkin                      100      19.0%\n",
      "474    pickup_truck                 99      18.9%\n",
      "475    butterfly                    98      18.7%\n",
      "476    streetcar                    98      18.7%\n",
      "477    mountains                    97      18.5%\n",
      "478    shark                        97      18.5%\n",
      "479    tulip                        97      18.5%\n",
      "480    lawn_mower                   96      18.3%\n",
      "481    squirrel                     95      18.1%\n",
      "482    group                        94      17.9%\n",
      "483    cloud                        94      17.9%\n",
      "484    dolphin                      94      17.9%\n",
      "485    leopard                      94      17.9%\n",
      "486    letter                       93      17.7%\n",
      "487    lizard                       93      17.7%\n",
      "488    whale                        93      17.7%\n",
      "489    pile                         92      17.5%\n",
      "490    fox                          92      17.5%\n",
      "491    flatfish                     90      17.1%\n",
      "492    possum                       90      17.1%\n",
      "493    trout                        90      17.1%\n",
      "494    cockroach                    89      17.0%\n",
      "495    lettering                    88      16.8%\n",
      "496    poppy                        88      16.8%\n",
      "497    spider                       88      16.8%\n",
      "498    bee                          87      16.6%\n",
      "499    skunk                        87      16.6%\n",
      "500    kangaroo                     86      16.4%\n",
      "501    beetle                       85      16.2%\n",
      "502    beaver                       84      16.0%\n",
      "503    orchid                       84      16.0%\n",
      "504    palm_tree                    84      16.0%\n",
      "505    snake                        84      16.0%\n",
      "506    wolf                         84      16.0%\n",
      "507    aquarium_fish                83      15.8%\n",
      "508    snail                        83      15.8%\n",
      "509    license                      81      15.4%\n",
      "510    crocodile                    81      15.4%\n",
      "511    porcupine                    81      15.4%\n",
      "512    raccoon                      81      15.4%\n",
      "513    otter                        80      15.2%\n",
      "514    cloudy                       79      15.0%\n",
      "515    maple_tree                   79      15.0%\n",
      "516    caterpillar                  78      14.9%\n",
      "517    pine_tree                    77      14.7%\n",
      "518    taken                        76      14.5%\n",
      "519    hamster                      75      14.3%\n",
      "520    shrew                        75      14.3%\n",
      "521    chimpanzee                   74      14.1%\n",
      "522    willow_tree                  74      14.1%\n",
      "523    worm                         73      13.9%\n",
      "524    bunch                        71      13.5%\n",
      "525    oak_tree                     71      13.5%\n",
      "526    sweet_pepper                 61      11.6%\n",
      "\n",
      "================================================================================\n",
      "Degree range: 61 - 525\n"
     ]
    }
   ],
   "source": [
    "# NOTE Edges per word\n",
    "print(\"=\" * 80)\n",
    "print(\"EDGES PER NODE (DEGREE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "graph = augmented_network['graph']\n",
    "\n",
    "# Get degree for each node\n",
    "degrees = dict(graph.degree())\n",
    "\n",
    "# Sort by degree (descending)\n",
    "sorted_degrees = sorted(degrees.items(), key=lambda x: -x[1])\n",
    "\n",
    "print(f\"Total nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"Total edges: {graph.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(degrees.values()) / len(degrees):.2f}\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Rank':<6} {'Word':<20} {'Degree':>10} {'% of Max':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "max_degree = sorted_degrees[0][1] if sorted_degrees else 1\n",
    "for rank, (word, degree) in enumerate(sorted_degrees, 1):\n",
    "    pct = (degree / max_degree) * 100\n",
    "    print(f\"{rank:<6} {word:<20} {degree:>10} {pct:>9.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"Degree range: {min(degrees.values())} - {max(degrees.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b9246",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "# TRAINING SKIPGRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c0c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 STARTING TRAINING RUN\n",
      "This may take a few minutes. We are running the full pipeline...\n",
      "================================================================================\n",
      "\n",
      "🔧 PUNCTUATION FILTER:\n",
      "  Removed: {'.', ',', \"'\", '<RARE>'}\n",
      "  Nodes: 526 → 523\n",
      "  Edges: 56,917 → 55,417\n",
      "\n",
      "Train edges: 49,875, Val edges: 5,542\n",
      "\n",
      "📊 SkipGramDataset Statistics:\n",
      "  Vocabulary size: 523\n",
      "  Positive pairs: 273,006\n",
      "  Negatives per positive: 8\n",
      "  Total samples per epoch: 2,457,054\n",
      "\n",
      "  Weight distribution:\n",
      "    Min: 0.071974\n",
      "    Mean: 1.000000\n",
      "    Median: 0.071974\n",
      "    Max: 16.642145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/voodoo/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 SkipGramDataset Statistics:\n",
      "  Vocabulary size: 523\n",
      "  Positive pairs: 172,754\n",
      "  Negatives per positive: 8\n",
      "  Total samples per epoch: 1,554,786\n",
      "\n",
      "  Weight distribution:\n",
      "    Min: 0.095653\n",
      "    Mean: 1.000000\n",
      "    Median: 0.095653\n",
      "    Max: 15.452855\n",
      "\n",
      "Training on cpu\n",
      "Vocab: 523, Embed dim: 64, Context: 2, Negatives: 8\n",
      "Regularization: dropout=0.0, weight_decay=0.0, label_smoothing=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train=2.2573  val=4.3771  lr=0.005000\n",
      "  → Best model (val_loss=4.3771), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02  train=2.0882  val=4.5376  lr=0.005000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03  train=2.0768  val=4.5649  lr=0.005000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04  train=2.0782  val=4.5204  lr=0.005000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05  train=2.0008  val=4.4210  lr=0.002500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06  train=1.9777  val=4.4415  lr=0.002500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07  train=1.9782  val=4.4138  lr=0.002500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08  train=1.9405  val=4.3601  lr=0.001250\n",
      "  → Best model (val_loss=4.3601), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09  train=1.9269  val=4.3542  lr=0.001250\n",
      "  → Best model (val_loss=4.3542), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  train=1.9278  val=4.3274  lr=0.001250\n",
      "  → Best model (val_loss=4.3274), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11  train=1.9224  val=4.3355  lr=0.001250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12  train=1.9247  val=4.3365  lr=0.001250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13  train=1.9242  val=4.3375  lr=0.001250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14  train=1.9025  val=4.3085  lr=0.000625\n",
      "  → Best model (val_loss=4.3085), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15  train=1.8997  val=4.3291  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16  train=1.9014  val=4.3122  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17  train=1.8991  val=4.2991  lr=0.000625\n",
      "  → Best model (val_loss=4.2991), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18  train=1.8951  val=4.3053  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19  train=1.8969  val=4.2983  lr=0.000625\n",
      "  → Best model (val_loss=4.2983), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20  train=1.8930  val=4.3023  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21  train=1.8968  val=4.2926  lr=0.000625\n",
      "  → Best model (val_loss=4.2926), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22  train=1.8943  val=4.3003  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23  train=1.8938  val=4.3070  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24  train=1.8966  val=4.3095  lr=0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25  train=1.8832  val=4.3019  lr=0.000313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26  train=1.8839  val=4.2831  lr=0.000313\n",
      "  → Best model (val_loss=4.2831), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27  train=1.8813  val=4.2907  lr=0.000313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28  train=1.8788  val=4.2880  lr=0.000313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29  train=1.8796  val=4.2888  lr=0.000313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30  train=1.8805  val=4.2849  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31  train=1.8750  val=4.2802  lr=0.000156\n",
      "  → Best model (val_loss=4.2802), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32  train=1.8780  val=4.2800  lr=0.000156\n",
      "  → Best model (val_loss=4.2800), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33  train=1.8785  val=4.2767  lr=0.000156\n",
      "  → Best model (val_loss=4.2767), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34  train=1.8739  val=4.2794  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35  train=1.8765  val=4.2742  lr=0.000156\n",
      "  → Best model (val_loss=4.2742), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36  train=1.8782  val=4.2790  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37  train=1.8763  val=4.2756  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38  train=1.8745  val=4.2714  lr=0.000156\n",
      "  → Best model (val_loss=4.2714), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39  train=1.8740  val=4.2761  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40  train=1.8776  val=4.2731  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41  train=1.8790  val=4.2681  lr=0.000156\n",
      "  → Best model (val_loss=4.2681), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42  train=1.8739  val=4.2755  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43  train=1.8732  val=4.2786  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44  train=1.8732  val=4.2789  lr=0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45  train=1.8725  val=4.2713  lr=0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46  train=1.8720  val=4.2778  lr=0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47  train=1.8709  val=4.2713  lr=0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48  train=1.8725  val=4.2718  lr=0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49  train=1.8718  val=4.2691  lr=0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50  train=1.8727  val=4.2742  lr=0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51  train=1.8706  val=4.2703  lr=0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52  train=1.8731  val=4.2753  lr=0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53  train=1.8727  val=4.2716  lr=0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54  train=1.8692  val=4.2679  lr=0.000010\n",
      "  → Best model (val_loss=4.2679), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55  train=1.8713  val=4.2719  lr=0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56  train=1.8745  val=4.2701  lr=0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57  train=1.8721  val=4.2734  lr=0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58  train=1.8669  val=4.2712  lr=0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59  train=1.8704  val=4.2717  lr=0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60  train=1.8687  val=4.2717  lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61  train=1.8691  val=4.2706  lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62  train=1.8658  val=4.2670  lr=0.000002\n",
      "  → Best model (val_loss=4.2670), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63  train=1.8678  val=4.2702  lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64  train=1.8714  val=4.2722  lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65  train=1.8740  val=4.2675  lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66  train=1.8704  val=4.2689  lr=0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67  train=1.8705  val=4.2669  lr=0.000001\n",
      "  → Best model (val_loss=4.2669), saved to best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68  train=1.8680  val=4.2709  lr=0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# --- Hyperparameters ---\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# We're using a smaller dimension (64 for speed).\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# A wide context_size (4) captures broader graph relationships.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# The batch size is large for GPU efficiency.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmented_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Dimension of the learned vectors\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Number of pairs per training step #NOTE adjusted this from 2048 to 32. idk\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#from 10               # Max epochs (will stop early) #NOTE I just reduced this to 10 cus 20 took so long\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#from 0.001      # AdamW learning rate\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_negative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 8 negatives per 1 positive\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use 10% of edges for validation\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#NOTE I've just changed this from 4 to 2 .4 would find things like \"Dog\" is connected to \"Bark\" -> \"Tree\" -> \"Building\" -> \"Sky\".\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- Regularization Stack ---\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#(Was 0.3)            # Prevent neuron co-adaptation\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#(Was 1e-4)      # L2 penalty to keep weights small\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#(Was 0.1)    # Prevent overconfidence (target 0.9, not 1.0)\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#(Was 5)           # Stop if val_loss doesn't improve for 5 epochs\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Auto-detects 'cuda' or 'cpu'\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# --- Training Summary ---\u001b[39;00m\n\u001b[1;32m     33\u001b[0m nodes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/PyTorch & AI CW/lab6.py:635\u001b[0m, in \u001b[0;36mtrain_embeddings\u001b[0;34m(network_data, embedding_dim, batch_size, epochs, learning_rate, num_negative, validation_fraction, context_size, dropout, weight_decay, label_smoothing, patience, device, save_plot)\u001b[0m\n\u001b[1;32m    633\u001b[0m val_pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(val_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (centers, contexts, negs) \u001b[38;5;129;01min\u001b[39;00m val_pbar:\n\u001b[1;32m    636\u001b[0m         centers, contexts, negs \u001b[38;5;241m=\u001b[39m centers\u001b[38;5;241m.\u001b[39mto(device), contexts\u001b[38;5;241m.\u001b[39mto(device), negs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    638\u001b[0m         batch_loss \u001b[38;5;241m=\u001b[39m model(centers, contexts, negs, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/PyTorch & AI CW/lab6.py:272\u001b[0m, in \u001b[0;36mSkipGramDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    270\u001b[0m center_idx, context_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs[idx]  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    271\u001b[0m center_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[center_idx]  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m excluded \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_to_idx[n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontexts[center_node]}  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    273\u001b[0m excluded\u001b[38;5;241m.\u001b[39madd(center_idx)  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    274\u001b[0m available \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m excluded], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)  \u001b[38;5;66;03m#*\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PyTorch & AI CW/lab6.py:272\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m center_idx, context_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs[idx]  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    271\u001b[0m center_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[center_idx]  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m excluded \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_to_idx\u001b[49m[n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontexts[center_node]}  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    273\u001b[0m excluded\u001b[38;5;241m.\u001b[39madd(center_idx)  \u001b[38;5;66;03m#*\u001b[39;00m\n\u001b[1;32m    274\u001b[0m available \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m excluded], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)  \u001b[38;5;66;03m#*\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Image, display\n",
    "from lab6 import train_embeddings\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 STARTING TRAINING RUN\")\n",
    "print(\"This may take a few minutes. We are running the full pipeline...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# We're using a smaller dimension (64 for speed).\n",
    "# A wide context_size (4) captures broader graph relationships.\n",
    "# The batch size is large for GPU efficiency.\n",
    "results = train_embeddings(\n",
    "    network_data=augmented_network,\n",
    "    embedding_dim=64,        # Dimension of the learned vectors\n",
    "    batch_size=32,           # Number of pairs per training step #NOTE adjusted this from 2048 to 32. idk\n",
    "    epochs=100, #from 10               # Max epochs (will stop early) #NOTE I just reduced this to 10 cus 20 took so long\n",
    "    learning_rate=0.005, #from 0.001      # AdamW learning rate\n",
    "    num_negative=8,          # 8 negatives per 1 positive\n",
    "    validation_fraction=0.1, # Use 10% of edges for validation\n",
    "    context_size=2,          #NOTE I've just changed this from 4 to 2 .4 would find things like \"Dog\" is connected to \"Bark\" -> \"Tree\" -> \"Building\" -> \"Sky\".\n",
    "\n",
    "    # --- Regularization Stack ---\n",
    "    dropout=0.0, #(Was 0.3)            # Prevent neuron co-adaptation\n",
    "    weight_decay=0.0, #(Was 1e-4)      # L2 penalty to keep weights small\n",
    "    label_smoothing=0.0, #(Was 0.1)    # Prevent overconfidence (target 0.9, not 1.0)\n",
    "    patience=20,   #(Was 5)           # Stop if val_loss doesn't improve for 5 epochs\n",
    "    device=None                        # Auto-detects 'cuda' or 'cpu'\n",
    ")\n",
    "\n",
    "# --- Training Summary ---\n",
    "nodes = results['nodes']\n",
    "embeddings = results['embeddings']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Learned embeddings for {len(nodes):,} words\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"\\n💡 Key features of this training run:\")\n",
    "print(f\"  • Punctuation filtering prevented 'hub poisoning'\")\n",
    "print(f\"  • Weighted sampling focused on important pairs\")\n",
    "print(f\"  • Regularization (Dropout, L2, Smoothing) prevented overfitting\")\n",
    "print(f\"  • A wide context (size=4) learned from the graph structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477d120",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
